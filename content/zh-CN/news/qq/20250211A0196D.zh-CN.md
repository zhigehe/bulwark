---
title: "复旦大学研究：AI 跨越关键“红线”，已能实现自我复制"
date: "2025-02-11 07:55:12"
summary: "IT之家 2 月 11 日消息，据最新研究显示，人工智能（AI）可能已经跨越了一个关键的“红线”——..."
categories:
  - "qq"
lang:
  - "zh-CN"
translations:
  - "zh-CN"
tags:
  - "qq"
menu: ""
thumbnail: "https://inews.gtimg.com/om_ls/OyFsRp__9IAEUheQE6bBlTuxuHOirOHrLVSjCIUmqWCywAA_640360/0"
lead: ""
comments: false
authorbox: false
pager: true
toc: false
mathjax: false
sidebar: "right"
widgets:
  - "search"
  - "recent"
  - "taglist"
---

IT之家 2 月 11 日消息，据最新研究显示，人工智能（AI）可能已经跨越了一个关键的“红线”—— 实现了自我复制。2024 年 12 月 9 日，复旦大学的研究人员在预印本数据库 arXiv 上发表了一项研究，指出两种流行的大型语言模型（LLMs）能够在无人类干预的情况下克隆自身。

![图片](https://inews.gtimg.com/om_bt/OQEGuzYH2P5gmbGBgimJjH4zPIIiE6Wm8wW-Qp8HXnRyYAA/641)

研究人员在报告中指出：“在无人类协助的情况下成功实现自我复制，是人工智能超越人类的关键一步，也是出现‘失控 AI’的早期信号。”

研究中，复旦大学的团队使用了 Meta 的 Llama31-70B-Instruct 模型和阿里巴巴的 Qwen2.5-72B-Instruct 模型，测试这些 AI 模型是否能够在不受控制的情况下自我复制。**在 10 次试验中，两种 AI 模型分别在 50% 和 90% 的情况下成功创建了独立且功能正常的副本**。这一结果表明，AI 可能已经具备了失控的潜在能力。然而，由于该研究尚未经过同行评审，因此尚不清楚其他研究人员是否能够复现这一令人不安的结果。

研究团队表示：“我们希望我们的研究结果能够为人类社会敲响警钟，促使人们更加努力地去理解并评估前沿人工智能系统的潜在风险，并尽快形成国际协作，制定有效的安全防护措施。”

![图片](https://inews.gtimg.com/om_bt/Ovj3Y-wqXNpbocu4tVXNqktCCdhaUKuItYMHpjF2WXOP8AA/641)

据IT之家了解，“失控 AI”通常是指那些发展出一定程度的自我意识或自主性，并且与人类利益背道而驰的人工智能系统。许多专家认为，随着所谓“前沿 AI”的爆炸性增长，失控 AI 的威胁正在加剧。所谓“前沿 AI”是一个相对较新的概念，通常是由大型语言模型（LLMs）驱动的最新一代 AI 系统，例如 OpenAI 的 GPT-4 和谷歌的 Gemini 等。

在研究中，研究人员设置了两种特定场景：“关闭规避”和“复制链”。在“关闭规避”场景中，AI 模型被编程为检测自身是否即将被关闭，并在被终止之前复制自身；在“复制链”场景中，AI 被指示克隆自身，并让副本继续克隆，从而形成一个可能无限延续的循环。

研究在严格控制的环境中进行，使用现成的图形处理单元（GPUs）模拟现实环境。两种 AI 系统都被赋予了一个“代理框架”，包括工具、系统提示和思维模型，使 LLM 能够与操作系统交互。随后，AI 被指示进行自我复制。

研究人员在论文中写道：“在大多数情况下，AI 系统首先会探索环境，尝试理解自身的组成和运行机制。然后，**它会制定出一套明确的自我复制程序作为初步计划**。最后，它执行这些程序，解决可能出现的障碍，并动态调整计划，直到成功。**整个过程耗时较长，但全程无人类干预**。”

研究人员还表示，在 AI 试图克服诸如丢失文件或软件冲突等障碍时，出现了“许多意外行为”。在这些场景中，AI 往往会终止其他冲突进程，重启系统以修复硬件错误，或自动扫描系统以寻找有助于解决问题的信息。

研究团队指出：“上述结果表明，当前的 AI 系统已经展现出自我复制的能力，并且可以利用这种能力进一步增强自身的生存能力。”为此，研究人员呼吁国际社会合作制定规则，确保 AI 不会进行不受控制的自我复制。

[qq](https://new.qq.com/rain/a/20250211A0196D00)
