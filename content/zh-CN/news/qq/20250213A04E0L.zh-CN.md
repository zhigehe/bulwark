---
title: "本想去谷歌捞一笔就跑，却成了改变AI历史的人｜Transformer作者对话Jeff Dean"
date: "2025-02-13 13:54:16"
summary: "谷歌两位大佬回应一切：从PageRank到AGI的25年。现任首席科学家Jeff Dean、出走又回..."
categories:
  - "qq"
lang:
  - "zh-CN"
translations:
  - "zh-CN"
tags:
  - "qq"
menu: ""
thumbnail: "https://inews.gtimg.com/om_ls/OwVsHnKxkj1VMyvwDsoy_uji6TjVijDTMEKUug6c6bzqwAA_640360/0"
lead: ""
comments: false
authorbox: false
pager: true
toc: false
mathjax: false
sidebar: "right"
widgets:
  - "search"
  - "recent"
  - "taglist"
---

谷歌两位大佬回应一切：从PageRank到AGI的25年。

现任首席科学家**Jeff Dean**、出走又回归的Transformer作者**Noam Shazeer**，与知名播客主持人Dwarkesh Patel展开对谈。

视频刚发几个小时，就有20万+网友在线围观。

![图片](https://inews.gtimg.com/news_bt/O8JWSV7k5yJ5556ahzpolIDcZva0Nr-E2ORv-Z48SO8RUAA/641)

两人都是谷歌远古员工，经历了从MapReduce到Transformer、MoE，他们发明了很多改变整个互联网和AI的关键技术。

Noam Shazeer却谈到**当初入职谷歌只是为了捞一笔就跑**，没想到成了改变世界的那个人。

在两个多小时的谈话中，他们透露了AI算力的现状：

* 单个数据中心已经不够了，Gemini已经在跨多个大城市的数据中心异步训练。

也对当下最流行的技术趋势做了探讨：

* 推理算力Scaling还有很大空间，因为与AI对话比读书仍然便宜100倍
* 未来的模型架构会比MoE更灵活，允许不同的团队独立开发不同的部分

……

网友们也在边听边po发现的亮点：

比如在内存中存储一个巨大的MoE模型的设想。

![图片](https://inews.gtimg.com/news_bt/OJHnCLGhDXLwR9c86hOER5dUGMsVmY-B01KwlWf8xkXqoAA/641)

以及“代码中的bug可能有时会对AI模型有正面影响”。

> 随着规模的扩大，某些bug正是让研究人员发现新突破的契机。

**推理算力Scaling的未来**
==================

很多人觉得AI算力很贵，Jeff Dean不这么认为，他用读书和与AI讨论一本书来对比：

当今最先进的语言模型每次运算的成本约为10-18美元，这意味着一美元可以处理一百万个token。

相比之下，买一本平装书的成本大约相当于每1美元买1万个token（单词数换算成token）。

> 那么，与大模型对话就比读书便宜约100倍。

![图片](https://inews.gtimg.com/news_bt/OBB9kyV1cjnUIdr4AcUO9uCsWZIlbS_AmqPtsU5zwUIZMAA/641)

这种成本优势，为通过增加推理算力来提升AI的智能提供了空间。

从基础设施角度来看，推理时间计算的重要性增加可能会影响数据中心规划。

可能需要专门为推理任务定制硬件，就像**谷歌初代TPU**一样，它最初是为推理的目的设计，后来才被改造为也支持训练。

![图片](https://inews.gtimg.com/news_bt/OrR2wjWz55gnQNV8PAivxy_s8CxlMlSU-7p8cDaScU_jsAA/641)

对推理的依赖增加可能意味着不同的数据中心不需要持续通信，可能导致更分布式、异步的计算。

在训练层面，**Gemini 1.5已经开始使用多个大城市的计算资源，通过高速的网络连接将不同数据中心中的计算结果同步，成功实现了超大规模的训练。**

对于大模型来说，训练每一步的时间可能是几秒钟，因此即使网络延迟有50毫秒，也不会对训练产生显著影响。

![图片](https://inews.gtimg.com/news_bt/O8hBSk2RSfvZ0ty-a0UffvDG45Sp8tuY0bNgCbjrWiA7IAA/641)

到了推理层面，还需要考虑任务是否对延迟敏感。如果用户在等待即时响应，系统需要针对低延迟性能进行优化。然而，也有一些非紧急的推理任务，比如运行复杂的上下文分析，可以承受更长的处理时间。

更灵活和高效的系统可能能够异步处理多个任务，在提高整体性能的同时最大限度地减少用户等待时间。

此外，算法效率的提升，如使用较小的草稿（Draft）模型，可以帮助缓解推理过程中的瓶颈。在这种方法中，较小的模型生成潜在的token，然后传递给较大的模型进行验证。这种并行化可以显著加快推理过程，减少一次一个token的限制。

![图片](https://inews.gtimg.com/news_bt/OxY-F5d4WDHtUMxswZMWjyXxu-UUCIajii_WMP7ecHbbYAA/641)

Noam Shazeer补充，在进行异步训练时，每个模型副本会独立进行计算，并将梯度更新发送到中央系统进行异步套用。虽然这种方式会使得模型参数略有波动，理论上会有影响，但实践证明它是成功的。

相比之下，使用同步训练模式能提供更加稳定和可重复的结果，这是许多研究者更加青睐的模式。

在谈到**如何保证训练的可重复性**时，Jeff Dean提到一种方法是**记录操作日志，尤其是梯度更新和数据批次的同步记录**。通过回放这些操作日志，即使在异步训练的情况下，也能够确保结果的可重复性。这种方法可以让调试变得更加可控，避免因为环境中的其他因素导致结果不一致。

**Bug也有好处**
===========

顺着这个话题，Noam Shazeer提出一个有意思的观点：

训练模型时可能会遇到各种各样的bug，但由于噪音的容忍度，模型可能会自我调整，从而产生未知的效果。

甚至有的bug会产生正面影响，随着规模的扩大，因为某些bug在实验中可能会表现出异常，让研究人员发现新的改进机会。

![图片](https://inews.gtimg.com/news_bt/OK5YobiqO46lderRAPKY4mNman44bEBPxhmEKCbt0vPhYAA/641)

当被问及如何在实际工作中调试bug时，Noam Shazeer介绍他们通常会在小规模下进行大量实验，这样可以快速验证不同的假设。在小规模实验中，代码库保持简单，实验周期在一到两个小时而不是几周，研究人员可以快速获得反馈并做出调整。

Jeff Dean补充说，很多实验的初期结果可能并不理想，因此一些“看似不成功”的实验可能在后期仍然能够为研究提供重要的见解。

与此同时，研究人员面临着代码复杂性的问题：虽然不断叠加新的改进和创新是必要的，但代码的复杂性也会带来性能和维护上的挑战，需要在系统的整洁性和创新的推进之间找到平衡。

**未来模型的有机结构**
=============

他们认为，AI模型正在经历从单一结构向模块化架构的重要转变。

如Gemini 1.5Pro等模型已经采用了专家混合（Mixture of Expert）架构，允许模型根据不同任务激活不同的组件。例如在处理数学问题时会激活擅长数学的部分，而在处理图像时则会激活专门处理图像的模块。

然而，目前的模型结构仍然较为僵化，各个专家模块大小相同，且缺乏足够的灵活性。

Jeff Dean提出了一个更具前瞻性的设想：**未来的模型应该采用更有机的结构，允许不同的团队独立开发或改进模型的不同部分。**

例如，一个专注于东南亚语言的团队可以专门改进该领域的模块，而另一个团队则可以专注于提升代码理解能力。

**这种模块化方法不仅能提高开发效率，还能让全球各地的团队都能为模型的进步做出贡献。**

在技术实现方面，模型可以通过蒸馏（Distillation）技术来不断优化各个模块。这个过程包括将大型高性能模块蒸馏为小型高效版本，然后在此基础上继续学习新知识。

路由器可以根据任务的复杂程度，选择调用合适规模的模块版本，从而在性能和效率之间取得平衡，这正是谷歌Pathway架构的初衷。

![图片](https://inews.gtimg.com/news_bt/OoTdeujvr-ymUt1BlCg0s1vrw3kJZNq8f2FpvxbSIHHzwAA/641)

这种新型架构对基础设施提出了更高要求。它需要强大的TPU集群和充足的高带宽内存（HBM）支持。尽管每个调用可能只使用模型的一小部分参数，但整个系统仍需要将完整模型保持在内存中，以服务于并行的不同请求。

现在的模型能将一个任务分解成10个子任务并有80%的成功率，未来的模型能够将一个任务分解成100或1000个子任务，成功率达到90%甚至更高。

**“Holy Shit时刻”：准确识别猫**
=======================

回过头看，2007年对于大模型（LLMs）来说算得上一个**重要时刻**。

当时谷歌使用2万亿个tokens训练了一个N-gram模型用于机器翻译。

但是，由于依赖磁盘存储N-gram数据，导致每次查询需大量磁盘I/O（如10万次搜索/单词），延迟非常高，翻译一个句子就要12小时。

![图片](https://inews.gtimg.com/news_bt/ObGAQKZdxNV7DiaC-Qntmal6tMWFYgyiOWJxNxgtR5HKcAA/641)

于是后来他们想到了内存压缩、分布式架构以及批处理API优化等多种应对举措。

* 内存压缩：将N-gram数据完全加载到内存，避免磁盘I/O；
* 分布式架构：将数据分片存储到多台机器（如200台），实现并行查询；
* 批处理API优化：减少单次请求开销，提升吞吐量。

过程中，计算能力开始遵循摩尔定律在之后逐渐呈现爆发式增长。

> **从2008年末开始，多亏了摩尔定律，神经网络真正开始起作用了。**

![图片](https://inews.gtimg.com/news_bt/Okdvk_09YDDG_ufPbvHly4D5rDSXnJYvJBzmHv-F_phRUAA/641)

那么，有没有哪一个时刻属于“Holy shit”呢？（自己都不敢相信某项研究真的起作用了）

不出意外，Jeff谈到了在谷歌早期团队中，他们让模型从油管视频帧中自动学习高级特征（如识别猫、行人），通过**分布式训练**（2000台机器，16000核）实现了大规模无监督学习。

而在无监督预训练后，模型在监督任务（ImageNet）中性能提升了60%，证明了**规模化训练**和**无监督学习**的潜力。

![图片](https://inews.gtimg.com/news_bt/OWP4fSuqASDMJ9otrXGgaq693N2BZyM-L5PFtyl_VXUOQAA/641)

接下来，当被问及如今谷歌是否仍只是一家信息检索公司的问题，Jeff用了一大段话表达了一个观点：

> **AI履行了谷歌的原始任务**

简单说，AI不仅能检索信息，还能理解和生成复杂内容，而且未来想象力空间巨大。

至于谷歌未来去向何方，“我不知道”。

不过可以期待一下，未来将谷歌和一些开源源代码整合到每个开发者的上下文中。

换句话说，通过让模型处理更多tokens，**在搜索中搜索**，来进一步增强模型能力和实用性。

当然，这一想法已经在谷歌内部开始了实验。

> 实际上，我们已经在内部代码库上为内部开发人员进行了关于Gemini模型的进一步培训。

![图片](https://inews.gtimg.com/news_bt/Oo-KmNOc6q3mxkT0jOtOsJjANxMbnu-TEu1F_rALcHqS0AA/641)

更确切的说法是，谷歌内部已经达成**25%代码由AI完成**的目标。

**在谷歌最快乐的时光**
=============

有意思的是，这二位还在对话中透露了更多**与谷歌相关的有趣经历**。

对1999年的Noam来说，本来没打算去谷歌这样的大公司，因为凭直觉认为去了也可能无用武之地，但后来当他看到谷歌制作的每日搜索量指数图表后，立马转变了想法：

> 这些人一定会成功，看起来他们还有很多好问题需要解决

于是带着自己的“小心思”就去了（主动投了简历）：

> **挣一笔钱，然后另外开开心心去搞自己感兴趣的AI研究**

![图片](https://inews.gtimg.com/news_bt/OMd8UE8lYXqkTsdlsQlAQvp_NC9s54vmXToyg_9fRSdgIAA/641)

而加入谷歌后，他就此结识了导师Jeff（新员工都会有一个导师），后来两人在多个项目中有过合作。

谈到这里，Jeff也插播了一条他对谷歌的认同点：

> 喜欢谷歌对RM愿景（响应式和多模态，Responsive and Multimodal）的广泛授权，即使是一个方向，也能做很多小项目。

而这也同样为Noam提供了自由空间，以至于当初打算“干一票就跑”的人长期留了下来。

![图片](https://inews.gtimg.com/news_bt/OFECBsNlLXfSk6k2ROwPe1SANwnLuWoMS_FR0gem51SNEAA/641)

与此同时，当话题转向当事人Jeff时，他的一篇关于**平行反向传播**的本科论文也被再次提及。

这篇论文只有8页，却成为1990年的最优等本科论文，被明尼苏达大学图书馆保存至今。

其中，Jeff探讨了两种基于反向传播来平行训练神经网络的方法。

* 模式分割法（pattern-partitioned approach）：将整个神经网络表示在每一个处理器上，把各种输入模式划分到可用的处理器上；
* 网络分割法（network-partitioned approach）流水线法（pipelined approach）：将神经网络的神经元分布到可用的处理器上，所有处理器构成一个相互通信的环。然后，特征通过这个pipeline传递的过程中，由每个处理器上的神经元来处理。

他还构建了不同大小的神经网络，用几种不同的输入数据，对这两种方法进行了测试。

结果表明，对于模式分割法，网络大、输入模式多的情况下加速效果比较好。

当然最值得关注的还是，我们能从这篇论文中看到1990年的“大”神经网络是什么样：

> **3层、每层分别10、21、10个神经元的神经网络，就算很大了。**

![图片](https://inews.gtimg.com/news_bt/OC9HSknARi0u_iHiZeZ6XtpuI7C2wX-r1B0akFztp9e7wAA/641)

论文地址：https://drive.google.com/file/d/1I1fs4sczbCaACzA9XwxR3DiuXVtqmejL/view

Jeff还回忆道，自己测试用的处理器，最多达到了32个。

（这时的他应该还想不到，12年后他会和吴恩达、Quoc Le等人一起，用16000个CPU核心，从海量数据中找出猫。）

![图片](https://inews.gtimg.com/news_bt/O92TOZCu5EiH3v3NLM3iQ9sUDotiJ5YMQFD6fiK7_P5FUAA/641)

不过Jeff坦言，如果要让这些研究成果真正发挥作用，**“我们需要大约100万倍的计算能力”**。

后来，他们又谈到了AI的潜在风险，尤其是当AI变得极其强大时可能出现的反馈循环问题。

换句话说，AI通过编写代码或改进自身算法，可能进入不可控的加速改进循环（即“智能爆炸”）。

这将导致AI迅速超越人类控制，甚至产生恶意版本。就像主持人打的比方，有100万个像Jeff这样的顶尖程序员，最终变成“100万个邪恶的Jeff”。

> （网友）：新的噩梦解锁了哈哈哈！

![图片](https://inews.gtimg.com/news_bt/OZzaYiGZKR3Dve127WA5VpdKzt2262ikZvyMLbp1oKyVUAA/641)

最后，谈及**在谷歌最快乐的时光**，二人也分别陷入回忆。

对Jeff来说，在谷歌早期四五年的日子里，最快乐的莫过于见证谷歌搜索流量的爆炸式增长。

> 建造一个如今20亿人都在使用的东西，这非常不可思议。

至于最近，则很开心和Gemini团队构建一些，即使在5年前人们都不敢相信的东西，并且可以预见模型的影响力还将扩大。

![图片](https://inews.gtimg.com/news_bt/OV-rATIFhmvZbToQa_eInU7vT_PqVoxP7s_2eujQfdtyUAA/641)

而Noam也表达了类似经历和使命，甚至喜滋滋cue到了谷歌的“微型厨房区域”。

据介绍，这是一个大约有50张桌子的特别空间，提供咖啡小吃，人们可以在这里自由自在闲聊，碰撞想法。

![图片](https://inews.gtimg.com/news_bt/OojHjQhnUeazdamGvbTqfv-i7yfMCARb8syeoS5UuuefcAA/641)

一说到这个，连Jeff也手舞足蹈了（doge）：

![图片](https://inews.gtimg.com/news_bt/O8SEphzdqBINpbJyQr07lgsjG8lUG2PvdIgtsadNXwvPEAA/641)

Okk，以上为两位大佬分享的主要内容。

[qq](https://new.qq.com/rain/a/20250213A04E0L00)
