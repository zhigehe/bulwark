---
title: "多模态版DeepSeek来了，北大出品，可用于机器人控制"
date: "2025-02-13 19:00:02"
summary: "机器人前瞻（公众号：robot_pro）作者 |  许丽思编辑 |  漠影机器人前瞻2月13日报道，..."
categories:
  - "qq"
lang:
  - "zh-CN"
translations:
  - "zh-CN"
tags:
  - "qq"
menu: ""
thumbnail: "https://inews.gtimg.com/om_ls/O96x7vflh9UQ60EF9DkUee-OUuiR_sxChXfkkgiO_80nEAA_640360/0"
lead: ""
comments: false
authorbox: false
pager: true
toc: false
mathjax: false
sidebar: "right"
widgets:
  - "search"
  - "recent"
  - "taglist"
---

机器人前瞻（公众号：robot\_pro）作者 |  许丽思编辑 |  漠影机器人前瞻2月13日报道，最近，北京大学联合香港科技大学团队基于自研全模态框架Align-Anything，将纯文本模态的Deepseek R1系列模型拓展至了图文模态，推出多模态版DeepSeek-R1，即Align-DS-V，它在部分视觉理解表现评测集上超越GPT-4o。联合研究团队中的北京大学对齐团队的指导老师为北京大学人工智能研究院助理教授杨耀东，同时也是北京具身智能初创公司灵初智能与北大共同成立的具身灵巧操作联合实验室首席科学家。在Deepseek R1发布后，研究团队在一周之内将Deepseek R1系列模型扩展至图文模态，并取得了优越的视觉理解表现。![图片](https://inews.gtimg.com/om_bt/OqVmdn8vdC5oMe92CRGxd16b6-OB41446-86-q48r6PU4AA/641)此外，团队还发现了模态穿透对于模型文本模态推理能力的提升效果。在DeepSeek R1的全模态化尝试中，团队发现，经过多模态训练之后，模型在文本模态任务上的表现有所提升，在科学任务、复杂推理、数学代码等方面的表现均有提升。![图片](https://inews.gtimg.com/om_bt/OcpT9yvjI79PXOoMDdXBSazQM98AUSOD78KD5khIbbc0YAA/641)Align-DS-V的多模态强推理能力是VLA模型（视觉语言动作模型）大脑端核心，且同样的后训练技术能应用于小脑端控制器微调，从而实现更高的成功率、泛化性和鲁棒性。目前，Align-Anything框架，以及DeepSeek-R1的多模态版本Align-DS-V，均已开源。一、Align-Anything框架，对齐全模态大模型与人类意图Align-Anything框架致力于使全模态大模型与人类意图和价值观对齐，全模态包括文生文、文生图、文图生文、文生视频等任意到任意的输入与输出模态，框架设计了具备高度的模块化、扩展性以及易用性的对齐训练框架，支持由文本、图片、视频、音频四大基本模态衍生出的任意模态模型对齐微调，并验证了框架对齐算法的实现正确性。该框架具有以下特点：高度模块化：对不同算法类型的抽象化和精心设计的API，用户能够为不同的任务修改和定制代码，以及定制化模型与数据集注册等高级扩展用法；支持跨任意模态模型的微调：包含对如LLaMA3.2、LLaVA、Chameleon、Qwen2-VL、Qwen2-Audio、Diffusion等跨越多种模态生成与理解的大模型的微调能力；支持不同的对齐方法：支持任意模态上的多种对齐算法，既包括SFT、DPO、PPO等经典算法，也包括ORPO, SimPO和KTO等新算法；支持多种开、闭源对齐评估：支持了30多个多模态评测基准，包括如MMBench、VideoMME等多模态理解评测，以及如FID、HPSv2等多模态生成评测。![图片](https://inews.gtimg.com/om_bt/OulhfwR4DN-jufydFuEsrH9EWtyr-GP-XRPaifIYDWCn8AA/641)同时，研究团队还发布首个全模态人类偏好数据集Align-Anything。与专注于单个模态且质量参差不齐的现有偏好数据集不同，Align-Anything提供了高质量的数据，包括了输入和输出中的任何模态，旨在提供详细的人类偏好注释以及用于批评和改进的精细语言反馈，从而实现跨模态的全面评估和改进。![图片](https://inews.gtimg.com/om_bt/OUluCPae-nJiM99dxEXmuIbm_gKHTa2MbeRMX--cTnx6kAA/641)二、扩展Deepseek R1的视觉模态，打造Align-DS-V在Align-Anything框架的基础上，团队研发了Align-DS-V。借鉴LLaVA的训练思路，通过训练投影层（Projector），团队将视觉编码器（Vision Encoder）输出映射到语言表征空间，从而扩展了Deepseek R1的视觉模态。为验证全模态推理大模型在垂域应用的能力，研发团队对Align-DS-V面向进行香港地区价值观的本地化对齐，令Align-DS-V适应粤语/英语/普通话混合语言输入，整合港铁动态、台风预警及八达通缴费等香港本土生活场景。在被图文询问到哪一款维他奶（香港地区的热门饮品）更加减脂时，Align-DS-V精确地选择了其中的低糖原味豆奶，并且也指出原味豆奶同样适合减脂饮用，为香港日常饮食选择提供便利。![图片](https://inews.gtimg.com/om_bt/OuQNKQ1aWfk3PJU1ix5R36S5ctQMkDcFyzc5EmMR_KF3cAA/641)在面对包含繁体字的图文数学问题时，Align-DS-V能够准确联动图文模态信息，使用严密而逐步的数学推导展示求解过程。![图片](https://inews.gtimg.com/om_bt/OyYkC81M1xrc4_-dlo4WFWTE7l1dsLd_giOBwFoPmAMb8AA/641)结语：DeepSeek的变革席卷具身智能据了解，在Align-DS-V的基础上，北大-灵初联合实验室已经着手在VLA（视觉语言动作模型）领域方面做更深度的探索。灵初智能正在研发的VLA模型，在大脑端利用多模态大模型进行对齐和微调，并向小脑端的控制器输出action token；而后，小脑端的控制器再根据输入的token和其他模态的信息，输出具体的机器人控制指令。这两个过程都需要运用针对多模态大模型的后训练和微调技术。北大-灵初联合实验室表示，Align-DS-V的多模态强推理能力是VLA模型大脑端的核心，接下来将利用多模态推理模型的跨模态穿透能力，实现action穿透，最终实现真正高效的VLA模型。同样的后训练技术也可以应用于小脑端控制器的微调，实现更高的成功率、泛化性和鲁棒性。值得注意的是，最近还有多家具身智能相关企业宣布在DeepSeek上做出尝试：优必选正在验证 DeepSeek 技术在人形机器人应用场景中的有效性，猎户星空的机器人AgentOS也接入了DeepSeek-R1，科大讯飞已在讯飞开放平台上线DeepSeek全系大模型……同时，还有传言称，Figure AI终止与OpenAI合作极有可能是因为正基于 DeepSeek-R1 开发自家机器人大模型、宇树科技与DeepSeek达成深度合作等。从技术研发到场景应用，这场由DeepSeek开源生态驱动的技术革命，正在加速人形机器人感知理解决策能力的进化，也有望在机器人“大脑”层面为企业降低门槛，推动更多的力量向更底层的运动控制“小脑”上集中，从而推动具身智能更快地从实验室迈向现实场景。附上开源地址：Align-Anything框架地址https://github.com/PKU-Alignment/align-anythingDeepDeek-R1多模态版本Align-DS-V地址：https://huggingface.co/PKU-Alignment/Align-DS-V

[qq](https://new.qq.com/rain/a/20250213A07MRK00)
