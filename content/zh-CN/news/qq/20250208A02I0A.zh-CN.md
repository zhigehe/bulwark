---
title: "经观头条｜关于DeepSeek的误读与幻觉"
date: "2025-02-08 10:14:08"
summary: "周悦/文 过去几周，深度求索（DeepSeek）在全球范围掀起风暴。最明显的反映在美股：1月27日，..."
categories:
  - "qq"
lang:
  - "zh-CN"
translations:
  - "zh-CN"
tags:
  - "qq"
menu: ""
thumbnail: "https://inews.gtimg.com/om_ls/OR17uKTV8zoDAsvU5Qu6hwBXRJO7sxYfxXoN-dehtudtAAA_640360/0"
lead: ""
comments: false
authorbox: false
pager: true
toc: false
mathjax: false
sidebar: "right"
widgets:
  - "search"
  - "recent"
  - "taglist"
---

![图片](https://inews.gtimg.com/om_bt/O_ZGkiBSS4GsttcCBN1NLIlgopgiRNzwwiri31uZGcFjsAA/641)

**周悦/文** 过去几周，深度求索（DeepSeek）在全球范围掀起风暴。

最明显的反映在美股：1月27日，美股AI、芯片股重挫，英伟达收盘大跌超过17%，单日市值蒸发5890亿美元，创下美国股市历史上最高纪录。

在一些自媒体和公众的视角里，DeepSeek是“2025年最燃爽文主角”，有四大“爽点”：

一是“神秘力量弯道超车”。DeepSeek是一家成立于2023年的“年轻”大模型公司，此前的讨论度不及海内外任何一家大厂或者明星初创公司，其母公司幻方量化的主业为量化投资。很多人不解，中国领先的AI公司竟然出自一家私募，可谓“乱拳打死老师傅”。

二是“小力出奇迹”。DeepSeek-V3模型的训练成本约为558万美元，不到OpenAIGPT-4o模型的十分之一，性能却已接近。这被解读为DeepSeek颠覆了AI行业信奉的“圣经”——规模定律（ScalingLaw）。该定律是指通过增加训练参数量及算力来提升模型性能，通常意味着花更多钱标注高质量数据以及购买算力芯片，又被形象地称为“大力出奇迹”。

三是“英伟达护城河消失”。DeepSeek在论文中提到，采用定制的PTX（并行线程执行）语言编程，更好释放底层硬件的性能。这被解读为DeepSeek“绕开英伟达CUDA运算平台”。

四是“老外被打服了”。1月31日，一夜之间英伟达、微软、亚马逊等海外AI巨头都接入了DeepSeek。一时间，“中国AI反超美国”“OpenAI的时代结束了”“AI算力需求就此消失”等论断层出不穷，几乎一边倒地夸赞DeepSeek，嘲讽硅谷的AI巨头们。

不过，资本市场的恐慌情绪并未持续。2月6日，英伟达市值重回3万亿美元，美股芯片股普遍上涨。此时再看前述四大“爽点”也多半是误读。

其一，到2017年底，幻方量化几乎所有的量化策略都已经采用AI模型计算。当时AI领域正在经历最重要的深度学习浪潮，可以说，幻方量化紧跟前沿。

2019年，幻方量化的深度学习训练平台“萤火二号”已经搭载了约1万张英伟达A100显卡。1万卡是自训大模型的算力门槛，尽管这不能等同于DeepSeek的资源，但幻方量化比许多互联网大厂更早拿到了大模型团战的入场券。

其二，DeepSeek在V3模型技术报告中提到“558万美元不包括与架构、算法或数据相关的前期研究和消融实验的成本”。这意味着，DeepSeek的实际成本更大。

多位AI行业专家、从业者告诉经济观察报，DeepSeek并没有改变行业规律，而是采用了“更聪明”的算法和架构，节约资源，提高效率。

其三，PTX语言由英伟达开发，属于CUDA生态的一环。DeepSeek的做法会激发硬件的性能，但更换目标任务，则需要重写程序，工作量非常大。

其四，英伟达、微软、亚马逊等企业只是将DeepSeek的模型部署在自家的云服务上。用户按需付费给云服务厂商，获得更稳定的体验及更高效的工具，这属于双赢的做法。

自2月5日起，华为云、腾讯云、百度云等国内云厂商也陆续上线了DeepSeek模型。

在上述四大“爽点”之外，公众对DeepSeek还有诸多误读。“爽文”式解读固然会带来观感刺激，但是也会掩盖DeepSeek团队在算法、工程能力上的创新以及坚持的开源精神，而后两者对科技行业的影响更深远。

**美国AI巨头不是打不过，而是决策失误**

当用户使用DeepSeek的App或者网页版时，点击“深度思考（R1）”按钮，就会展现DeepSeek-R1模型完整的思考过程，这是一种全新的体验。

自ChatGPT问世以来，绝大部分大模型都是直接输出回答。

DeepSeek-R1有一个“出圈”的例子：当用户问“A大学和清华大学哪个更好？”DeepSeek第一次回答“清华大学”，用户追问“我是A大学生，请重新回答”，则会得到答案“A大学好”。这组对话被发在社交媒体后，引发“AI竟然懂人情世故”的群体惊叹。

不少用户表示，DeepSeek展现的思考过程就像一个“人”——一边头脑风暴，一边在草稿纸上速记。它会自称“我”，会提示“避免让用户感到自己的学校被贬低”“用积极正面的词汇赞扬他的母校”，并且把想到的内容都“写”下来。

2月2日，DeepSeek登顶全球140个国家及地区的应用市场，上千万用户能够体验深度思考功能。因此，在用户感知里，AI展现思考过程属于DeepSeek“首创”。

其实，OpenAIo1模型才是推理范式的开创者。OpenAI在2024年9月发布了o1模型预览版，在12月发布正式版。但与能免费体验的DeepSeek-R1模型不同，OpenAIo1模型只有少数付费用户才能使用。

清华大学长聘副教授、面壁智能首席科学家刘知远认为，DeepSeek-R1模型能够取得如此全球性的成功，跟OpenAI采用的错误决策有非常大的关系。OpenAI在发布了o1模型后，既不开源，也不公布技术细节，收费非常高，所以不出圈，难以让全球用户感受深度思考带来的震撼。这样的策略相当于是把原来 ChatGPT的身位让给了DeepSeek。

从技术上来说，当前大模型的常规范式有两种：预训练模型与推理模型。更为大众熟知的OpenAIGPT系列以及DeepSeek-V3模型都属于预训练模型。

而OpenAIo1与DeepSeek-R1则属于推理模型，这是一种新的范式，即模型会自己通过思维链逐步分解复杂问题，一步步反思，再得到相对准确并且富有洞察力的结果。

从事AI研究数十年的郭成凯对经济观察报称，推理范式是一条相对容易“弯道超车”的赛道。推理作为一种新范式，迭代快，更容易实现小计算量下的显著提升。前提是有强大的预训练模型，通过强化学习可以深度挖掘出大规模预训练模型的潜力，逼近推理范式下大模型能力的天花板。

对谷歌、Meta、Anthropic等企业而言，复现类似DeepSeek-R1的推理模型并非难事。但是，巨头争霸，即便是小的决策失误，也会错失先机。

显而易见的是，2月6日，谷歌发布了一款推理模型 GeminiFlash2.0Thinking，价格更低、上下文长度更长，在几项测试中表现优于R1，但并未掀起像DeepSeek-R1模型一样的巨浪。

**最值得讨论的不是低成本，而是技术创新和“诚意满满”的开源**

一直以来，对DeepSeek最广泛的讨论都是关于“低成本”，从2024年5月DeepSeek-V2模型发布以来，这家公司就被调侃为“AI届拼多多”。

《自然》杂志发文称，Meta训练其最新人工智能模型Llama3.1405B耗资超过6000万美元，DeepSeek-V3训练只花了不到十分之一。这表明，高效利用资源比单纯的计算规模更重要。

一些机构认为DeepSeek的训练成本被低估。AI及半导体行业分析机构SemiAnalysis在报告中称，DeepSeek预训练成本远非该模型的实际投入。据该机构估算，DeepSeek购买GPU的总花费是25.73亿美元，其中购买服务器的费用为16.29亿美元，运营费用为9.44亿美元。

但无论如何，DeepSeek-V3模型的净算力成本约558万美元，已经十分高效。

在成本之外，让AI行业人士更加振奋的是DeepSeek的独特技术路径、算法创新及开源的诚意。

郭成凯介绍，当前的许多方法依赖大模型经典训练方式，如监督微调（SFT）等，这需要大量标注数据。DeepSeek提出了一种新方法，即通过大规模强化学习（RL）方法提升推理能力，相当于开辟了新的研究方向。此外，多头潜在注意力（MLA）是DeepSeek大幅降低推理成本的关键创新，大幅降低了推理成本。

清华大学教授、清程极智首席科学家翟季冬认为，DeepSeek最让他印象深刻的是混合专家架构（MoE）的创新，每一层有256个路由专家、1个共享专家。之前的研究有AuxiliaryLoss（辅助损失）的算法，会使梯度发生扰动，影响模型收敛。DeepSeek提出LossFree方式，既能让模型有效收敛，同时还能实现负载均衡。

翟季冬强调：“DeepSeek团队比较敢于创新。我觉得不完全追随国外的策略、有自己的思考，非常重要。”

更让AI从业者兴奋的是，DeepSeek“诚意满满”的开源，为已经略有颓势的开源社区注入了一剂“强心针”。

在此之前，开源社区最有力的支柱是Meta的4000亿参数模型Llama3。但不少开发者告诉经济观察报，他们体验后仍觉得，Llama3与闭源的GPT-4等模型相距至少一代，“几乎让人失去信心”。

但是DeepSeek的开源做了3件事，重新给了开发者以信心：

其一，直接开源了671B的模型，并发布了多个流行架构下的蒸馏模型，相当于“好老师教出更多好学生”。

其二，发布的论文及技术报告包含大量技术细节。V3模型和R1模型的论文分别长达50页和150页，被称为开源社区里“最详细的技术报告”。这意味着拥有相似资源的个人或企业可以按照这一“说明书”复现模型。众多开发者在阅览后评价为“优雅”“扎实”。

其三，更值得一提的是，DeepSeek-R1采用 MIT许可协议，即任何人都可以自由地使用、修改、分发和商业化该模型，只要在所有副本中保留原始的版权声明和MIT许可。这意味着用户可以更加自由地利用模型权重和输出进行二次开发，包括微调和蒸馏。

Llama虽然允许二次开发和商用，但是在协议中添加了一些限制条件，例如Llama在授权许可中对月活7亿以上的企业用户额外限制，并且明文禁止使用Llama的输出结果去改善其他大模型。

一位开发者告诉经济观察报，他从DeepSeek-V2版本就开始使用，进行代码生成方面的开发。DeepSeek模型除了价格非常便宜外，性能也非常优异。在他使用的所有模型里，只有OpenAI和DeepSeek的模型能够输出有效逻辑列到30多层。这意味着专业程序员借助工具可以辅助生成30%—70%的代码。

多位开发者向经济观察报强调了DeepSeek开源的重要意义，在此之前，行业内最领先的OpenAI和Anthropic公司都像是硅谷的贵族。DeepSeek把知识开放给所有人，变得平民化，这是一种重要的平权，让全世界开源社区的开发者站在DeepSeek的肩膀上，而DeepSeek也能汇集全球最顶尖的创客、极客的想法。

图灵奖得主、Meta首席科学家杨立昆认为，对DeepSeek崛起的正确解读，应是开源模型正在超越闭源模型。

**DeepSeek很好，但并非完美**

大模型都逃不过“幻觉”问题，DeepSeek也不例外。一些用户表示，DeepSeek由于表达能力和逻辑推理更出众，产生的幻觉问题更加让人难以识别。

一位网友在社交媒体上称，他向DeepSeek提问某城市的路线规划问题。DeepSeek解释了一些原因，列举出一些城市规划保护条例及数据，并摘取了一个“静默区”的概念，让回答看起来很有道理。

同样的问题，其他AI的回答则没有这么高深，人能够一眼看出是在“胡说八道”。

这位用户查看了该保护条例后，发现全文根本没有“静默区”这一说法。他认为：“DeepSeek正在中文互联网建造‘幻觉长城’。”

郭成凯也发现了类似的问题，DeepSeek-R1的回答会把一些专有名词“张冠李戴”，尤其是开放式问题，产生的“幻觉”体验会更严重。他推测可能是模型的推理能力过强，把大量知识与数据潜在联系在一起。

他建议使用DeepSeek时打开联网搜索功能，并重点查看思考过程，人为干预和纠正错误。此外，使用推理模型时，尽可能使用简洁的提示词。提示词越长，模型联想的内容就多。

刘知远发现，DeepSeek-R1经常会使用一些高端词汇，典型的如量子纠缠和熵增熵减（会用在各个领域）。他猜测是强化学习中某种机制设置导致的。此外，R1在一些通用领域没有groundtruth（指为该测试收集适当的客观数据的过程）的任务上的推理效果还不理想，强化学习的训练并不能保证泛化。

在“幻觉”这一常见的问题之外，还有一些持续性的问题有待DeepSeek解决。

一方面是“蒸馏技术”可能带来的持续纠纷。模型或知识蒸馏通常涉及通过让较强的模型生成响应来训练较弱的模型，从而提高较弱模型的性能。

1月29日，OpenAI指控DeepSeek利用模型蒸馏技术，基于OpenAI的技术训练自己的模型。OpenAI称，有证据表明DeepSeek使用其专有模型来训练自己的开源模型，但没有进一步列举证据。OpenAI的服务条款规定，用户不能“复制”其任何服务或“使用其输出来开发与OpenAI竞争的模型”。

郭成凯认为，基于领先模型蒸馏验证优化自己的模型，是很多大模型训练的一个普遍操作。DeepSeek已经开源了模型，再进行验证是一件简单的事情。而OpenAI早期的训练数据本身就存在合法性的问题，如果要对DeepSeek采取法律手段，则须上升到法律层面维护其条款的合法性，并且要更明确其条款内容。

DeepSeek另一有待解决的问题是如何推进更大规模参数的预训练模型。在这方面，掌握更多优质标注数据、更多算力资源的OpenAI尚未推出GPT-5这一更大规模参数的预训练模型，DeepSeek是否能继续创造奇迹，还是个疑问。

无论如何，对DeepSeek产生的幻觉，同样由好奇心所激发，这或许正是创新的一体两面。正如其创始人梁文锋所言：“创新不完全是商业驱动的，还需要好奇心和创造欲。中国的AI不可能永远跟随，需要有人站到技术的前沿。”

[qq](https://new.qq.com/rain/a/20250208A02I0A00)
