---
title: "苹果机器人新突破：赋予机器人“灵动”手势，和人类一样有魅力"
date: "2025-02-08 18:27:53"
summary: "腾讯科技《AI未来指北》特约作者 周小燕编辑 郑可君人与人之间的交流，除了语言，还可以通过手势、面部..."
categories:
  - "qq"
lang:
  - "zh-CN"
translations:
  - "zh-CN"
tags:
  - "qq"
menu: ""
thumbnail: "https://inews.gtimg.com/om_ls/OBHcDOZvd5Uw3TxucnZsQpgV2zp81HsqdVWLH8TkZvRq4AA_640360/0"
lead: ""
comments: false
authorbox: false
pager: true
toc: false
mathjax: false
sidebar: "right"
widgets:
  - "search"
  - "recent"
  - "taglist"
---

**腾讯科技《AI未来指北》特约作者 周小燕**

**编辑 郑可君**

人与人之间的交流，除了语言，还可以通过手势、面部表情和身体语言传递更多的信息。同样，机器人在与人类互动时，如果能够通过合适的手势和动作来表达情感，互动就会变得更加流畅和自然。

在现阶段，机器人虽然能够执行一些简单的手势动作，但这些动作通常是预设的，缺乏灵活性，无法根据不同的社交情境做出合适的反应。

**为了让人形机器人能够“读懂”动作，并且做动作时更自然，苹果人工智能研究人员发布了EMOTION框架**，它通过大型语言模型（LLM）和视觉语言模型（VLM），帮助机器人生成自然且符合情境的手势动作。

![图片](https://inews.gtimg.com/news_bt/O-mPw_PoSUquXuv-jMRQ6he_Rni_zpVAQAWbTSlNAde6oAA/641)

这样，机器人不仅能够做出“竖大拇指”或“挥手”等简单手势，还能根据不同的交流环境做出动态、富有表现力的手势，提升与人类的互动体验。

#### **EMOTION框架，让机器人也能为你竖起“大拇指”点赞**

EMOTION利用**大型语言模型（LLM）和视觉语言模型（VLM）**，让机器人能够理解并生成符合社交情境的手势。

根据苹果发表的论文显示，EMOTION框架大语言模型主要用于动作生成序列，通过调用**OpenAI GPT-4的API**进行文本补全来实现；VLM 主要使用的是Vision Transformers技术，帮助机器人从图像中提取重要信息，比如 3D 关键点，如同人眼凭借识别物体形状来理解其信息，机器人也能借助这种技术解读图片中的信息。

![图片](https://inews.gtimg.com/news_bt/Oh7e_m6l-Lmoc8xetptU2i7L7BZVy3WIHLkQwgGUPw_eAAA/641)

这些模型如同机器人的“社交大脑”。当机器人看到某个情景或接收到语言指令时，框架会通过“学习”这些信息，迅速生成合适的动作。

例如，看到某个人正在解题，**机器人会自动做出一个“竖大拇指”的手势，以表达鼓励和支持。**

这种能力的核心，在于机器人能根据社交环境自动“判断”最合适的动作，而不是依赖于预设的、固定的程序。

![图片](https://inews.gtimg.com/news_bt/GR8UeuxMvErhap_t-UXoTcfeopW7nFHj6yFLpn9subm-8AA/0)EMOTION框架下机器人的环境判断

那么，EMOTION是如何做到这一点的呢？

其实，它的工作流程就像是一个高度智能化的“手势工厂”。

**首先，机器人会通过“上下文学习”，即理解当前情境所传达的情感和信息。**当机器人接收到社交情境的输入时，比如一个语言指令，或者是机器人对周围环境的视觉观察，它会根据这些理解，通过模型生成符合情境的动作序列，就像画家根据不同的题材创作出各具风格的作品。

这一过程的关键在于**动态生成**。与传统的预设手势不同，EMOTION能够根据每一次交互的独特情境，生成与之最匹配的手势。例如，机器人可能会因为不同的情绪表达或任务需求，做出不同的手势，而不是重复固定的动作。这使得机器人不仅是一个简单的“执行者”，更像是一个可以与人类进行富有表现力对话的伙伴。

**其次，尽管EMOTION本身已能够生成多样化的手势，但人类的反馈仍然是提升互动质量的关键。**通过引入EMOTION++版本，机器人能够在与人类的互动中，学习和调整自己的表现。这个过程就像是一个艺术家不断在作品上进行打磨，直到它达到了更为自然和合适的效果。

![图片](https://inews.gtimg.com/news_bt/GBH4rFnSrjg86Fl2CyjnJfhU7ghIaisQIlipfXE0JtvDMAA/0)EMOTION框架下，机器人在人类反馈之下的手势互动 

在这个过程中，人类可以通过反馈告诉机器人哪些动作看起来更自然、哪些手势更易于理解。例如，假如机器人做出一个“听”的手势，但这个手势的手臂位置不够直观，可能让人误解为“停止”或“拥抱”，那么通过人类的调整建议，机器人可以优化这个手势，最终呈现出更加合适的表达。

通过这种**人类反馈**的持续优化，机器人生成的动作变得更加符合人类的期望和社交习惯，从而提升了机器人在互动中的自然性和可理解度。最终，EMOTION框架不仅使机器人能够做出正确的手势，还能使它们在与人类交流时表现得更富有“情感”和“智慧”。

#### **手势魅力大比拼：机器人 VS 人类**

为了验证EMOTION框架的有效性，研究团队设计了一项用户研究，目的就是对比**EMOTION生成的手势**与**人类亲自表演的手势**在自然性和理解度上的差异。

这就像是在评选谁的“表演”更具魅力——是机器人还是人类？

在实验中，参与者观看了由机器人和人类演示的手势视频，然后对这些手势的自然性和可理解性进行了评分。结果显示，EMOTION生成的手势与人类手势之间并没有显著的差异。简而言之，机器人做的手势在大多数情况下与人类的动作一样自然、容易理解。

![图片](https://inews.gtimg.com/news_bt/Oxixzj1GpDsEyCupu7SVcIrFFF--C-b617In6U3wVQVPUAA/641)

（用户对生成的机器人表现性行为的可理解性和自然性的评分，按手势分类。*\*和\*\*表示统计学意义，其中\**表示p < 0.05，\*\*表示p < 0.01。误差条表示均值的标准误差（SE））

**（ORACLE**：表示由人类示范的手势，是实验中的对比组，作为参考。**EMOTION**：表示由EMOTION框架生成的手势，基于大型语言模型（LLM）和视觉语言模型（VLM）进行生成的机器人手势。**EMOTION++**：表示通过EMOTION框架生成的手势，但在此基础上引入了人类反馈（优化版本），以进一步提升手势的自然性和可理解度。）

**然而，尽管EMOTION表现良好，它仍有一些不足，特别是在某些细节的处理上。部分手势，尤其是更为复杂或细腻的动作，仍需要进一步的优化和调整。**

那么，是什么影响了这些手势的自然性和理解度呢？

**第一点，研究表明，手势的细节在这其中起着至关重要的作用，也就是硬件限制。**

比如，当前的机器人硬件可能无法灵活地模拟所有细腻的手势。例如，**手的位置**和**手指的姿势**直接决定了手势是否能够准确传达意图。就像你在用手势传达“OK”时，如果手指的弯曲程度不对，别人可能会误解成“歪了的OK”或者完全不理解你的意图。

![图片](https://inews.gtimg.com/news_bt/OBG8CzenY4ImOl05o-_-59vw9-1QAfwdnyX2d-5nKQPhYAA/641)机械手的运动序列

**第二点，动作模式**也起到了重要作用。想象一下，如果你让机器人做一个“停止”的手势，动作的“流畅性”和“直观性”决定了观众能否立刻理解这个动作。如果动作过于生硬，或是路径不够直接，就容易造成误解。因此，EMOTION需要精细调整手势的流畅度和准确度，确保每一个动作都能迅速且准确地传达出正确的信息。

**第三个挑战是计算时间。**生成每个动作序列需要一定的时间，而目前的计算速度可能不能满足实时互动的需求。为了让机器人能够像人类一样在自然对话中快速做出反应，计算时间需要进一步减少。

![图片](https://inews.gtimg.com/news_bt/O4x3m68CKU0rXP5_YuOzwejXabJFYij_Gc9ZXGwCg7D9QAA/641)通过多次运行实验，计算了每次生成手势所需的平均时间，并考虑了计算中可能的波动（标准差）；Initial sequence（初始序列），Single-round HF（单轮人类反馈）

通过实验统计显示：

第一，生成初始动作序列的时间普遍较长。例如，对于“Thumbs-up”手势，初始序列生成的时间为28.7秒，而其他手势的时间也大致在24秒到33秒之间。这表明，机器人在初次生成动作时，需要较多的计算和处理时间。

第二，在加入人类反馈后，计算时间普遍缩短。例如，“Thumbs-up”手势在人类反馈后的时间降至24.4秒，相较于初始生成的时间，明显减少。这意味着，通过人类反馈优化手势序列后，机器人能够更高效地调整和改进动作。

面对这些挑战，未来的解决方向包括**优化硬件设计**，提升机器人的关节和手指灵活性；同时，加速计算过程，利用更高效的算法和本地化计算，缩短响应时间。

随着EMOTION框架的进一步发展，机器人不仅能在实验环境中表现出色，还可以在**不同的应用场景**中发挥更大的潜力。例如，家庭助手、教育机器人、医疗机器人等，未来都能借助EMOTION框架，进行更加自然和富有表现力的互动。

[qq](https://new.qq.com/rain/a/20250208A075V500)
