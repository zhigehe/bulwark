---
title: "潘攻愚：从四个角度全面驳斥美方对DeepSeek的质疑和污蔑"
date: "2025-02-12 07:34:02"
summary: "【文/观察者网专栏作者 潘攻愚】 DeepSeek“小扣发大鸣”，半年多的时间，不但从LLM通用模..."
categories:
  - "qq"
lang:
  - "zh-CN"
translations:
  - "zh-CN"
tags:
  - "qq"
menu: ""
thumbnail: "https://inews.gtimg.com/om_ls/OHHCVujwUMP3e8M5rVdfzf9cQy5uvKGeoc13t5-CI8IZAAA_640360/0"
lead: ""
comments: false
authorbox: false
pager: true
toc: false
mathjax: false
sidebar: "right"
widgets:
  - "search"
  - "recent"
  - "taglist"
---

【文/观察者网专栏作者 潘攻愚】

DeepSeek“小扣发大鸣”，半年多的时间，不但从LLM通用模型的V2迭代到了V3，而且进一步推出了主打推理能力的R1模型。从训练成本、架构调整和开源模式等多个维度技惊全球，引发了一股山呼海啸般的赞誉。春节期间大洋彼岸资本市场的大幅震荡以及开年后国内“DeepSeek概念股”的大涨，让这一现象持续成为坊间热议的焦点。

DeepSeek的成功，顺应了pre-training到推理的AI大模型的必然演化过程。DeepSeek的崛起为何是顺天应时之举？不妨先来看两段话。

去年2月下旬，英伟达CEO黄仁勋接受美国科技媒体Wired采访时说：“英伟达今天的业务可能是40%的推理和60%的训练，这是一件好事，因为这让你意识到AI终于成功了。如果英伟达的业务是90%的训练和10%的推理，你可以说AI仍处于早期研究阶段。”

去年12月，OpenAI的CFO Sarah Friar在接受科技媒体《信息》（The Information）采访时说：“OpenAI的ChatGPT Pro开放给C端用户的套餐每月200美元，实在是太便宜了，它合理的价格应该是每月2000美元。”进一步结合她上下文采访的言外之意，她主要是说OpenAI“心善”，秉承一股AI为大众平权服务的道义感，才没把价格搞得那么高。今天，他们这种伪善的画皮在DeepSeek R1开源模型面前彻底被撕下。

这两段话相当有代表性，一个指向AI技术应用的演进方向，一个则事关AI推训模式落地的商业化问题，这两个层面的问题相互缠绕，互为表里。

就在OpenAI牵头搞“星际之门”，将算力的Scale Law延伸到了民间资本市场和国家投资领域，试图把AI产业和美国国运绑定之时，DeepSeek对其做了一个釜底抽薪式的叙事消解。

众声喧哗之下，来自大洋彼岸的质疑，甚至是带有恶意性质的诋毁同样值得关注。

分析美国AI大模型行业某些头面人物带有惊慌失措心理的评论，可以进一步深化我们对DeepSeek到底真正打到了对方哪些痛处的认知。大洋彼岸的详细分析数据和质疑声音，以知名半导体咨询机构Semianalysis总裁Dylan Patel和Anthropic的CEO Dario Amodei为代表性，这两家的文章在中文互联网世界被翻译后大量转载。

![图片](https://inews.gtimg.com/om_bt/OcSXeu0-kidgNtsQe9Szn5bbuUFApWt_-f-jTjNSRjXrgAA/641)

Anthropic的CEO Dario Amodei

他们主要从GPU囤货、成本测算、非技术性营销、以及模型数据蒸馏不合规等四个角度，试图告诉公众DeepSeek的突破其实没那么“硬核”。

**一、摇唇鼓舌DeepSeek囤货“敏感性”高端GPU**

按照Semianaylsis的测算，“DeepSeek大致拥有10000张H800 GPU芯片、10000张H100 GPU芯片，以及大量H20 GPU芯片”。

Dario Amodei在长文中转述了Semianaylsis的测算，认为DeepSeek手上拥有的用于训练和推理的Hopper架构的英伟达GPU卡（阉割版和非阉割版都算在内）差不多有5万张，这个量和美国主要头部的AI模型训练机构如OpenAI、Deepmind等差距在两三倍左右，结合基于合成数据（synthetic data generation）和强化学习进行推理能力提升的后训练（post-training）方法，他认为DeepSeek本来就站在巨人的肩膀上，又用了巨量的GPU，才有了今天的成果。

为什么Dario Amodei要用Semianaylsis的数据给自己拉大旗扯虎皮呢？

因为Dario Amodei心中有一个所谓的AI训练成本的“摩尔定律法”——每一年大约能降三到四倍，如果用强化学习的方法进行推理架构调整，可以把成本降到六至八倍，但这个就是降成本的极限了。按照这种成本测算假说推断，DeepSeek有五万张Hopper卡。

![图片](https://inews.gtimg.com/om_bt/OadjkagFigy5qbhAhj5XZV2x_FpDuzzkW6uXcx6keTpZgAA/641)

那么，如果我们进一步追问，Semianaylsis认为DeepSeek手上有这么多高端GPU卡，他们是怎么算出来的？他们采用了一种类似归谬法的推理：Anthropic单单训练一个Claude 3.5 Sonnet的成本就高达数千万美元，如果DeepSeek有如此神之一手能强力降本，Anthropic何必煞费苦心去找亚马逊融资数亿呢？

有关Anthropic到底是怎么花费投资人的钱的问题，也许马斯克手下的DOGE（政府效率部）更有兴趣回答。相比微软、谷歌一派，代表云服务商亚马逊一派的Anthropic CEO按耐不住跳出来写长文的主要原因之一，是深刻觉察到在十万到百万级GPU基础上的生态进行推训，他们的Claude系列总价格是最高的，总性价比也是最低的。

![图片](https://inews.gtimg.com/om_bt/OOsXjg5MgGAi0_G0okJNp8L2rpRfTM5RJKxmAPFtUj6_MAA/641)

DeepSeek合法拥有的H800，相比H100，主要是阉割了NVLink的通信带宽；H20虽然也是阉割版，单卡算力仅有H100的20%，但H20可以通过多卡堆叠模式，其HBM容量（96GB）甚至高于A100/H100（80GB）。换言之，H20的显存带宽可以让DeepSeek的Decode阶段每生成1个Token所需时间低于A100和H100。

DeepSeek把阉割版用出了禁运版所没有的功效，让Dario Amodei居然发出了应该对中国大陆进一步加强GPU管制的恶意言论，这也许才是他抨击DeepSeek的目的。

从话语体系上讲，Semianalysis用Anthropic缺乏公允性的AI模型训练成本反推DeepSeek有可能绕开管制，非法持有高端GPU，而Anthropic反过来用Semianalysis建立在沙堆之上的推论来论述DeepSeek在成本问题上并无过人之处，**这其实是一个合谋式的循环论证。**

---

**二、DeepSeek隐藏了总成本（TCO）参数？**

Semianalysis和Anthropic对DeepSeek总成本的推断，还涉及到除了GPU采购之外的因素，诸如优化架构、处理数据、支付员工薪资等等，而这恰恰是我们最不太需要花费心思去反驳的。

通常意义上，H100的云租赁成本不包括电力成本，在数据中心实际托管的IT设备的成本与占地面积、园区环境和政策支持密切相关。

从未到中国进行过实地调研的Semianalysis，依据美国行情判断DeepSeek的API服务成本也是欠妥当的。

美国本土的云服务和大模型部署合作也相当复杂。与OpenAI自己的API相比，更多客户选择了微软进行公共和私有实例的推理，微软当年非常聪明地用自己的云服务积分置换对OpenAI的“天使轮投资”；而亚马逊喜欢把他们的SageMaker平台说成是客户在云上创建、训练和部署模型的好工具，但自己却用英伟达的Nemo云原生框架代替Sagemaker，来开发他们的模型。

相比Semianalysis对DeepSeek R1模型通过MLA（Multi-head Latent Attention）优化KV Cache机制的分析，他们对DeepSeek托管、运维和员工薪资的分析更像是一种臆测。

**三、DeepSeek赢在了营销？**

相比缺乏扎实一手调研和推论依据的成本估算，更让人匪夷所思的是，无论是Semianalysis还是Dario Amodei都用了不少的篇幅阐述了DeepSeek的“营销”手段，包括但不限于R1模型在实战中会首先向用户展示推理的思路框架，以及DeepSeek R1故意把发布时间踩点特朗普就职典礼等等。Semianalysis总裁Dylan Patel在近日的视频节目中，更是指出DeepSeek的营销胜在一个“快”上，比如说半年多以前急于推出成熟度欠奉的V2模型，意在炒作。

无利不起早的海外大厂已经用实际行动反击了这种“营销”说：从1月25日到2月1日，AMD的MI300X GPU、英伟达NIM微服务、英特尔Gaudi 2D Al加速器，均纷纷表示支持和接入DeepSeek V3/RI/Janus模型。如果DeepSeek没有展示出足够的技术实力，这些大厂为何要配合DeepSeek“营销”呢？

Semianalysis可能忽视了一个事实：2022年年底OpenAI急于推出的ChatGPT就是走了先占坑位然后再调试的路线，谷歌的Bard（现在已经改名Gemini）晚了一步被OpenAI抢了先手，就在于其创始团队担忧这种聊天机器人会抢夺搜索引擎市场从而影响谷歌营收，毕竟对谷歌来说，依靠搜索引擎导入的广告收入占了大头。

这一次，OpenAI在压力之下推出了全新的免费o3-mini（有趣的是，o3也在模仿R1展示推理思维链），可见“创新者困境”的魔咒和营销无关，这是一种涌浪式的推陈出新竞争法，指责DeepSeek以快取胜是毫无道理的。

从另一个层面上看，为什么OpenAI以及Anthropic的同推理模型不展示具体的推理思路呢？展示推理链路真的是一种营销吗？

OpenAI和Anthropic冠冕堂皇的理由是优化用户体验界面，避免信息过载。但这个问题其实触及到这几家公司深层次顾虑，一方面是模型的内部工作机制（如微调策略、特定任务的优化方法）可能会让竞争对手进行逆向工程，而且保持黑盒化的推理过程也避免了外界过分渲染这些工具的黑历史——从一开始，ChatGPT就很有争议性地不断爬取《纽约时报》、《华尔街日报》等公众媒体和数据资源进行语料训练，其合规性经营一再遭受质疑，并一度走到法律诉讼层面。

![图片](https://inews.gtimg.com/om_bt/ONayWE80bgYAkYUrkAe2U7lQBRHBYixmY4500FCet0Y2YAA/641)

由此可见，OpenAI、谷歌和Anthropic这些本来通过营销起家的AI模型公司无法效仿DeepSeek所谓的“营销大法”，非不为而实不能。

**结语：模型蒸馏是DeepSeek给全人类的美好馈赠**

Semianalysis总裁Dylan Patel和Anthropic 的CEO Dario Amodei对DeepSeek评述还有一个共性，就是认为R1远不如V3有趣，其主要论据是R1很可能用了模型蒸馏。

在保证模型性能与效率的同时，推动AI技术的普惠化，将其变为水和电一样的公共产品，模型数据蒸馏和用户知识蒸馏是一种必然之路，它不仅优化了资源利用，加速模型向本地部署和端侧推理迁移，对构建可持续、高效的AI生态具有重要意义。

OpenAI团队创立就是对谷歌AI商业化路线的一种逆反，奥尔特曼和马斯克当时秉承了一种为全人类寻找AGI途径的愿景才取了“OpenAI”这个名字，如今OpenAI变成“CloseAI”其实已经偏离了初心。

Dario Amodei抨击DeepSeek搞蒸馏有侵犯知识产权风险。但如前所述，这几家美国大厂都是吃到了数据时代红利，在《纽约时报》反应过来要搞法律诉讼之前先把语料数据“窃取”了过去，吃下去怎么可能再吐出来？

曾几何时，艰深晦涩的AI技术曾是学院派们的禁脔。英伟达的CUDA软件开发者系统平台，当初让先驱者们有机会在商业市场中一试身手。很快，AI的重心从斯坦福大学、多伦多大学和加州理工等转移到了初创公司中。

辛顿和李飞飞加入了谷歌，吴恩达去了百度，奥尔特曼和他闹宫斗的苏茨克维等等一起创办了OpenAI，他们一起把AI带向了公众视野。

一切的AI生产要素的流动，其实是一种人才、软硬件技术以及资本市场的变相“蒸馏”。本来就是脱胎自OpenAI的Anthropic也是用户知识蒸馏的最大受益者。

前一段时间李飞飞团队“50美元”复刻DeepSeek-R1，此举恰恰蕴藏着梁文峰们的美好愿景——推动知识与信息的平权，AI应成为造福全人类的公共产品。

![图片](https://inews.gtimg.com/om_bt/OksoOftI-mJpUg44g3Xqz3lJombYMsELqcM9QB1iwLGZ0AA/641)

**本文系观察者网独家稿件，文章内容纯属作者个人观点，不代表平台观点，未经授权，不得转载，否则将追究法律责任。关注观察者网微信guanchacn，每日阅读趣味文章。**

[qq](https://new.qq.com/rain/a/20250212A01DMZ00)
