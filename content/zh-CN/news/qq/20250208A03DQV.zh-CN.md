---
title: "超越CoT！微软剑桥中科院提出MVoT，直接可视化多模态推理过程"
date: "2025-02-08 11:33:28"
summary: "新智元报道  编辑：KingHZ【新智元导读】近日，微软和剑桥大学公布推理新方法：多模态思维可视..."
categories:
  - "qq"
lang:
  - "zh-CN"
translations:
  - "zh-CN"
tags:
  - "qq"
menu: ""
thumbnail: "https://inews.gtimg.com/om_ls/O3X2VvCbfrR4KWHXAy5x7EK-5mV4gNIapvKQIws033ZI0AA_640360/0"
lead: ""
comments: false
authorbox: false
pager: true
toc: false
mathjax: false
sidebar: "right"
widgets:
  - "search"
  - "recent"
  - "taglist"
---

### 图片

### --- **新智元报道**

编辑：KingHZ
##### **【新智元导读】**近日，微软和剑桥大学公布推理新方法：多模态思维可视化MVoT。新方法可以边推理，边「想象」，同时利用文本和图像信息学习，在实验中比CoT拥有更好的可解释性和稳健性，复杂情况下甚至比CoT强20%。还可以与CoT组合，进一步提升模型性能。

  
大模型也学会了「空间想象力」？还可以自己解释自己？在大语言模型（LLMs）和多模态大语言模型（MLLMs）中，思维链（CoT）在复杂推理方面非常有效。然而，对于复杂的**空间推理**，CoT表现不佳。但人类的认知能力不仅限于语言，还能够同时用词语和图像推理。受这一机制的启发，来自微软研究院、剑桥大学和中科院的研究人员，在思维链提示的基础上，提出了空间推理（spatial reasoning）新范式：多模态思维可视化（MVoT）。

![图片](https://inews.gtimg.com/om_bt/O1K4zrRf9KDeF3T4iJZcqDcK_KCsCd91VxN0BMm-93u1sAA/641)

论文地址：https://arxiv.org/pdf/2501.07542将思维链（CoT）扩展到多模态模型，已有的方法尽管能够处理文本和图像，但或者严重依赖于独立的视觉模块或外部工具，难以适应更复杂的空间推理任务；或者可视化太过简化，推理过程难以理解。论文作者Chengzu Li在X上解释MVoT的核心设计理念：「MVoT超越了思维链(CoT)，可以让AI利用生成的视觉图像去想象它的思考。通过融合语言和视觉推理，MVoT使复杂问题的解决变得更加直观、可更具解释性、更加强大。」![图片](https://inews.gtimg.com/om_bt/Ot5R1LtnGAnSFoHY9Dij-ZIb9KLlZYT7cpqgzvCiaPWjoAA/641)具体而言，MVoT要微调自回归多模态大语言模型（MLLM）。为了提升推理过程的可视化质量，引入了token差异损失，弥补了分别训练的分词器（tokenizer）的差距。文章亮点：

* 多模态思维可视化（MVoT）将文本与视觉统一在推理过程中，将自然生成视觉思维作为推理过程的一部分。
* 在Chameleon-7B中实现了MVoT，并在自回归多模态大语言模型（MLLM）中引入了token差异损失（token discrepancy loss），以弥补分别训练的文本分词器和图像分词器之间的差距。
* 实验结果表明，MVoT在复杂场景中比思维链（CoT）更优的适应性和稳健性。
* MVoT和CoT组合可以进一步提高性能上限。

架构

给定一个多模态输入序列，模型需要生成交织的多模态思维，作为推理过程的组成部分，并最终生成最终答案。

设![](https://inews.gtimg.com/om_bt/OHD6SBw4JWC5rgexNnIq5ygiHmtYwDbgX4xP6J4qvXSCgAA/641)表示一个预训练的多模态大语言模型（MLLM），其参数为θ，x表示多模态输入序列，z和v分别表示**语言思维**序列和**图像思维**序列。

![图片](https://inews.gtimg.com/om_bt/O1_CzcuVIdAVCdug_p08eSgg6cw59RBQWlYdBTOlKE92MAA/641)

在多跳（multi-hop）空间推理任务中，给定输入x，思维链（CoT）提示生成中间步骤![](https://inews.gtimg.com/om_bt/OJDnvLMTwpZlccOOs0NVhekEksoHKUEKkP78M7S8ww_vYAA/641) 其中每个样本基于输入和之前生成的步骤顺序采样。最终的输出基于所有先前的步骤得出。MVoT通过为每个中间步骤添加图像v^i可视化来增强这一过程，然后根据先前的步骤![](https://inews.gtimg.com/om_bt/OSAdQ_EbZIwjwOEXLQOnIsRFO7TtDiiLYBWHCvT_TIBtkAA/641)和可视化![](https://inews.gtimg.com/om_bt/O6QWrTQJX-tDWurdAVWBqRUYVLek3VloYEXBrb_fJM8Q8AA/641)采样后续步骤，如图1所示。

![图片](https://inews.gtimg.com/om_bt/ONuk-S5LUO_hhH9PksnaUaphTTw0kND3uNoRwVJDHRMEgAA/641)

图1：多模态思维可视化（MVoT）推理过程与其他方法的对比多模态思维可视化（MVoT）让多模态大语言模型（MLLMs）能在不同模态之间生成交织的推理轨迹。传统的CoT仅依赖于语言思维，而MVoT则通过促进视觉思维来可视化推理轨迹。这个**推理范式类似于人类的认知方式**，能够无缝地在文字和图像之间进行思维。

训练

多模态序列建模如图3所示，使用Chameleon的架构，利用统一的Transformer来处理图像和文本token。
==========================================================

![图片](https://inews.gtimg.com/om_bt/OzuZ4JQ8UnfYNqDAv4WxHwW8xK9FtsbnkywUYnD0EG44YAA/641)该架构集成了两个tokenizer：图像tokenizer使用离散的码本（codebook）将输入图像编码为一系列图像token；文本tokenizer则将文本数据映射为相应的token序列。这些token序列被连接在一起并由因果Transformer模型处理。

**损失函数**
--------

因果Transformer模型利用下一个token预测目标进行微调，同时两个tokenizer在整个过程中保持冻结状态。训练的损失函数要同时考虑图像token差异损失![](https://inews.gtimg.com/om_bt/OWa0nW7njJ74v2_TKoZ21BuT1NF7J-VwrYRaQK1ILRgDUAA/641)，以及文本token和图像token的交叉熵损失![](https://inews.gtimg.com/om_bt/OlTf207-Ym1tbuIVK2tIKVBSMT-aAomNVzqNwmowbpAFMAA/641)，定义如下。![图片](https://inews.gtimg.com/om_bt/OKtH9sva6R25Rw_6jfUIJvlXBFnkmSmk7SDUZ1PHuEK2AAA/641)其中token差异损失![](https://inews.gtimg.com/om_bt/OWa0nW7njJ74v2_TKoZ21BuT1NF7J-VwrYRaQK1ILRgDUAA/641)要对与真实标签显著偏离的token施加惩罚，弥合了语言建模与视觉嵌入空间之间的差距，同时确保梯度的连续性。

实验结果

**有效性实验**
---------

作者在三个动态空间推理任务中进行大量实验，验证了MVoT的有效性。MAZE和MINIBEHAVIOR聚焦于与空间布局的交互，而FROZENLAKE强调在动态环境中的细粒度模式识别。实验结果表明，MVoT在任务中的表现具有竞争力，在高难度的FROZENLAKE场景中，**MVoT的表现比传统的思维链（CoT）高出了20%多**。

![图片](https://inews.gtimg.com/om_bt/OZ3ZR0Hkytt9tmjKeBh3oAnYXnVdvn6iSQkqv0exZWX6IAA/641)

不同系统变体在任务中的实验结果。三个模拟任务的实验结果表明，Direct存在过拟合问题，准确率约为70%。GPT-4o的表现更差。相比之下，MVoT展现出不断的改进。在MAZE和MINIBEHAVIOR上，MVoT的准确率超过90%，可与CoT相媲美。而在FROZENLAKE上，MVoT的准确率为85.60%，优于Direct和CoT。**这表明MVoT比CoT拥有更好的稳定性和稳健性。**此外，**MVoT还提供了语言和视觉形式的中间推理状态，可以更清晰、更直观地理解推理过程**。

**定性分析**
--------

图4展示了FROZENLAKE中生成图像的正确与错误示例。可视化生成的错误分类如下：(1)错误可视化（Wrong Visualization）：生成的可视化内容不准确。

(2)多余图形（Redundant Patterns）：在预期修改区域外可视化了不必要或无关的图形。

![图片](https://inews.gtimg.com/om_bt/OExEBpbVfGWeRv7-p2eefWa7nUDqtvPG5XbjBzKlmkt-0AA/641)

图4：定性分析示意图。此外，与MAZE和MINIBEHAVIOR相比，在FROZENLAKE任务中，观察到随着模式复杂度的增加，生成图像的细节经常会变得模糊。在重建的图像与原始图像之间也观察到类似的差异。这种变异性经常导致细粒度细节的丢失或扰动，反映了MLLM在表达能力上的局限性。

**定量分析**
--------

为了评估生成的视觉推理的质量，基于已识别的错误类型定义了自动化评估指标：

* **可视化准确率（V-Acc.）**：衡量在网格中对应于下一步操作的预期修改是否被准确可视化。
* **可视化模式冗余度（V-Red.）**：评估目标修改区域之外是否存在非预期的视觉模式。
* **可视化正确步骤（V-Steps）**：操作序列中前k个连续正确可视化的平均长度。
* **可视化正确比率（V-Ratio）**：操作序列中前k个连续正确可视化的平均比例。

作者报告了MAZE和MINIBEHAVIOR中可视化位置的定量结果，如下所示。

![图片](https://inews.gtimg.com/om_bt/OhkPKBlssFCYsWhmKSfCXxqwVPzUVK6mOFhE6yT8nlYYgAA/641)

表3：token差异损失对MVoT视觉思维定量指标的影响

上图中，**最佳结果**以加粗形式标出。带有↑的指标表示值越高性能越好，反之亦然。

**Token差异损失函数分析**
-----------------

Token差异损失![](https://inews.gtimg.com/om_bt/OWa0nW7njJ74v2_TKoZ21BuT1NF7J-VwrYRaQK1ILRgDUAA/641)提高了可视化的准确性并减少了冗余。如表3所示，token差异损失![](https://inews.gtimg.com/om_bt/OWa0nW7njJ74v2_TKoZ21BuT1NF7J-VwrYRaQK1ILRgDUAA/641)增强的MVoT能够生成高度准确且模式冗余最小的可视化内容。即使在递归生成场景中，在推理过程中依然实现了95%的平均正确和连续可视化。相比之下，缺少token差异损失会显著降低生成质量：没有![](https://inews.gtimg.com/om_bt/OWa0nW7njJ74v2_TKoZ21BuT1NF7J-VwrYRaQK1ILRgDUAA/641)的MVoT经常会生成多余图形，且未能准确捕捉状态转换。这些结果与图像编辑场景中的发现一致，如图5所示，图中展示了MAZE在不同训练周期的定量指标。

![图片](https://inews.gtimg.com/om_bt/O6m5pveDDuFTeJzSr33cQhR6LGGZhJP6Ja1-pFFkN254sAA/641)

图5：MAZE在不同训练周期的定量指标

**MVoT和CoT的组合**
---------------

**MVoT在推理中与CoT的能力可以互相补充**。正如作者Chengzu Li所言：「MVoT不会取代CoT，而是提升了CoT。通过组合MVoT和CoT，多模态推理和语言推理的协同作用解锁了性能上限，证明两种推理范式可能比一种更好！」![图片](https://inews.gtimg.com/om_bt/O4hvK38sXSwSMZz7CZvfYgPOHCsVxLMGQZkhyEb0kYe-MAA/641)在两种方法的组合中，如果MVoT或CoT中的任一方法生成了正确的预测，则认为该数据点正确。如表4所示，在MAZE和MINIBEHAVIOR上，上限性能达到了接近100%的准确率；在FROZENLAKE上，达到了92%的准确率。

![图片](https://inews.gtimg.com/om_bt/O_ZjGFdA6elTFl__tRbkKW4qxX8Rg96l9aYA6txFFK9-wAA/641)

表4：通过组合CoT和MVoT在三个任务中的预测所达到的性能上限。文中也讨论了消融实验，并在附录中给出了更多的实验细节。当然，这项研究也有局限性，作者建议借鉴扩散模型中的图像生成技术，作为未来改进的方向。此外，在推理过程中，显式生成可视化会引入计算开销。为了解决这一问题，作者倡导进一步研究使用更少token的紧凑的图像表示，以降低可视化生成的计算成本。

作者介绍

![图片](https://inews.gtimg.com/om_bt/Owc2YouAyCr_GV6S69wDhUVMRLWR5Xt7skigKv5JEd010AA/641)

共一作者Chengzu Li在微软研究院实习时参与了全程工作。目前，他是剑桥大学语言技术实验室的计算、认知与语言学博士生。在攻读博士学位之前，他在剑桥大学计算机科学系获得了高级计算机科学硕士学位。他本科就读于西安交通大学自动化专业。![图片](https://inews.gtimg.com/om_bt/OXkEgC02PfPjyz7OSrziDjdM3Zeo8nIV9moL3IR5VrrLAAA/641)共一作者Wenshan Wu， 目前是微软亚洲研究院（MSRA）的高级研究软件开发工程师。之前，曾在腾讯担任软件工程师。她从中国科学院获得了硕士学位。参考资料：

https://arxiv.org/abs/2501.07542

https://x.com/li\_chengzu/status/1879168974988173573

[qq](https://new.qq.com/rain/a/20250208A03DQV00)
