---
title: "推理模型新路线开源！与DeepSeek截然不同，抛弃思维链不用人类语言思考"
date: "2025-02-11 15:12:26"
summary: "梦晨 发自 凹非寺量子位 | 公众号 QbitAI开源推理大模型新架构来了，采用与Deepseek-..."
categories:
  - "qq"
lang:
  - "zh-CN"
translations:
  - "zh-CN"
tags:
  - "qq"
menu: ""
thumbnail: "https://inews.gtimg.com/om_ls/OSnPT3OHRIfhdq54w1sbsN_zz2ZvPz24BAACeBfr-pspYAA_640360/0"
lead: ""
comments: false
authorbox: false
pager: true
toc: false
mathjax: false
sidebar: "right"
widgets:
  - "search"
  - "recent"
  - "taglist"
---

梦晨 发自 凹非寺  
量子位 | 公众号 QbitAI

开源推理大模型新架构来了，采用**与Deepseek-R1/OpenAI o1截然不同的路线**：

抛弃长思维链和人类的语言，直接**在连续的高维潜空间用隐藏状态推理**，可自适应地花费更多计算来思考更长时间。

![图片](https://inews.gtimg.com/news_bt/OvsoS1DJUFi7LiNEplHWko923eyDAUOQTAw0ZYekM_tVUAA/641)

例如问题：Claire每天早餐都会做一个3个鸡蛋的煎蛋卷。她在4周内会吃多少个鸡蛋？

从新模型Huginn的思考轨迹可视化中，可以看到对数字3等重要token不断旋转，最终收敛到正确答案对应的位置，但在不关键的人物名字Cla-ire上没有这个现象。

![图片](https://inews.gtimg.com/news_bt/O-Y9D6t4emWQnN-9JPIBKI-UWCCLuVG9x3j_212yC9woUAA/641)

除旋转之外还能观察到更多丰富的几何模式，研究团队认为这表明**该模型正在独立学习利用潜空间的高维性质以新的方式做推理**。

由于不使用长思维链推理范式，新方法还有几个额外优势：

* 不需要任何专门的训练数据
* 可以在很小的上下文窗口下工作
* 能捕捉到难以用语言表达的推理类型

研究来自马克思普朗克研究所、马里兰大学等团队，他们使用美国橡树岭实验室的Frontier超算完成训练实验，用到8个AMD GPU节点（4096块GPU），没有使用英伟达体系。

![图片](https://inews.gtimg.com/news_bt/OC6NaeJpoYoSKeQP_BFD3Xkbt-PKmQgIajriIsd20tmsAAA/641)

**新架构给Transformer加入循环模块**
-------------------------

新架构仍然围绕Decoder-only的Transformer block构建，但分为三段：

* **Prelude（前奏）：**使用多个transformer层将输入数据嵌入到潜空间中
* **Recurrent Block（循环块）：**循环计算单元，在潜在空间中修改状态
* **Coda（尾声）：**从潜空间解码，并包含模型的预测头

![图片](https://inews.gtimg.com/news_bt/Oc-gvf5rHq61tDt6-ahEQV1q9KqcGXEtV4dxWU2kd9ybcAA/641)

在训练期间为每个输入序列分配随机数量的迭代次数。同时为了在训练时保持较低的计算和内存，只反向传播循环单元的最后k次迭代。

研究中可视化了模型在潜在空间中的推理轨迹，发现了这些有趣现象：

* 对一些简单token，模型的隐状态会快速收敛到稳定点
* 但对一些关键token，如数学问题中的数字”3”，隐状态会形成复杂的圆形轨道
* 还有一些token的隐状态会沿特定方向”滑动”，可能用于计数循环次数

![图片](https://inews.gtimg.com/news_bt/OfIEIK7rd38adBNinKe0At6y7kNXj_qOQjNf9bZhEfmR4AA/641)

论文一作**Jonas Geiping**透露，他们的算力只够一次大规模训练，也就是最后发布的3.5B参数的Huginn模型，在800B tokens数据上预训练。

没有post/mid-training过程，但可以与7B参数、在2-3T tokens数据上训练的开源模型能力相匹配。

另外算上循环模块中的计算，3.5B参数的模型训练时的计算量相当于传统的32B模型。

![图片](https://inews.gtimg.com/news_bt/OCrdhOkLOuUGL1hC8afDwDo9nA-RMegLjqLuDhqXtvmY4AA/641)

有人猜测OpenAI o3使用了类似的方法，通过循环来达到近似无限上下文，并且控制高中低三种推理时间设置。

![图片](https://inews.gtimg.com/news_bt/OrtLDPbFhNX2nLi9Fmz8W_VamaTZm4Rwsi1C6fZqb0xbkAA/641)

有OpenAI研究员已经注意到这个工作，把论文读完了还在线捉bug。

![图片](https://inews.gtimg.com/news_bt/Ow_VzK1ZwbqDZ6DHwAXqc9XJ4yz3GGePuw3ZwtDem9p8gAA/641)

也已经有人准备根据DeepSeek-R1开源的方法尝试新思路，同时保留潜空间思考的推理能力，和CoT思考的可读性。

![图片](https://inews.gtimg.com/news_bt/OOt1nN9250D6OuPpPmbRoT5nmegfe9gwdnpiBGSMsIP9oAA/641)

论文：  
https://arxiv.org/abs/2502.05171模型：  
https://huggingface.co/tomg-group-umd/huginn-0125代码：  
https://github.com/seal-rg/recurrent-pretraining

参考链接：  
[1]https://x.com/tomgoldsteincs/status/1888980680790393085[2]https://x.com/jonasgeiping/status/1888985929727037514

[qq](https://new.qq.com/rain/a/20250211A05ANM00)
