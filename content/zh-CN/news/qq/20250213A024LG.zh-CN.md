---
title: "爆火的DeepSeek遭遇成本、技术质疑，未来算力还重要吗？"
date: "2025-02-13 09:50:13"
summary: "TechWeb文/ 卞海川近期，DeepSeek在AI大模型领域引发热议，凭借其惊人的性能表现和低成..."
categories:
  - "qq"
lang:
  - "zh-CN"
translations:
  - "zh-CN"
tags:
  - "qq"
menu: ""
thumbnail: "https://inews.gtimg.com/om_ls/OsouDhgWXBOLTRzSPmDazZ4BRlZkjiZVZouhR4S2ohY-AAA_640360/0"
lead: ""
comments: false
authorbox: false
pager: true
toc: false
mathjax: false
sidebar: "right"
widgets:
  - "search"
  - "recent"
  - "taglist"
---

![图片](https://inews.gtimg.com/om_bt/OsYtHi_Xy-31q9tAuAU3DwZsKsYkYiKjOvNGl-hewEAGAAA/641)

**TechWeb文/ 卞海川**

近期，DeepSeek在AI大模型领域引发热议，凭借其惊人的性能表现和低成本训练模式，迅速吸引了全球关注，且热度一直不减。但随之而来的，对其成本、技术以及为未来作为大模型基础设施的算力也引发了争议。

**DeepSeek陷成本误区？**

提及成本，DeepSeek发表的原始报告中有详细解释这笔成本的计算：“在预训练阶段，每兆个token上训练DeepSeek-V3仅需要180K H800 GPU小时，也就是说，在我们拥有2048个H800 GPU的丛集上需要3.7天。因此，我们的预训练阶段在不到两个月的时间内完成，耗费2664K GPU小时。加上上下文长度扩充所需的119K GPU小时和后制训练所需的5K GPU小时，DeepSeek-V3的完整训练仅需2.788M GPU小时。假设H800 GPU的租赁价格为每GPU小时2美元，我们的总训练成本仅为557.6万美元。”

对此，近日知名的SemiAnalysis公开发布的《DeepSeek Debates: Chinese Leadership On Cost, True Training Cost, Closed Model Margin Impacts》的报告中称：DeepSeek论文中提到的600万美元成本仅指预训练运行的GPU成本，这只是模型总成本的一小部分，他们在硬件上的花费远高于5亿美元。例如为了开发新的架构创新，在模型开发过程中，需要投入大量资金来测试新想法、新架构思路，并进行消融实验。开发和实现这些想法需要整个团队投入大量人力和GPU计算时间。例如深度求索的关键创新—多头潜在注意力机制（Multi-Head Latent Attention），就耗费了数月时间。

无独有偶，据外媒报道，李飞飞等斯坦福大学和华盛顿大学的研究人员以不到50美元的费用，使用了16张英伟达H100 GPU，耗时26分钟就完成了训练，成功“打造”出了一个名为s1-32B的人工智能推理模型。

有业内观点认为，DeepSeek模型低成本并不客观。

**武汉大学计算机学院教授、中国人工智能学会心智计算专委会副主任蔡恒进对TechWeb表示，DeepSeek在生成模型的成本在报告中已经写的很清晰了，过于纠结前期的投入，有多少实际成本是不重要的，特别是对于国内产业成本很低的情况下，前期做研究的成本也会比美国低很多。**

**蒸馏业内通行做法  
是非公婆各有理**

除了上述的成本外，业内对于DeepSeek争议最大的还有就是其是否使用了蒸馏技术，如果使用，究竟使用了谁家的？

最先提出质疑的是，是在DeepSeek R1发布之初，OpenAI和微软均向媒体证实，已掌握疑似DeepSeek通过“蒸馏”（distillation）技术，利用OpenAI专有模型来训练其AI大模型。这使得DeepSeek能够以较低的成本在特定任务上达到类似的效果。OpenAI虽未进一步提供证据的细节，但根据其服务条款，用户不得“复制”任何OpenAI的服务，或“利用输出结果开发与OpenAI竞争的模型”。

**对于DeepSeek爆火之下，马斯克一直罕见地并未发表评论，却在最近点赞了一则推文。**推文中提到了DeepSeek大量依赖模型蒸馏技术，需要借助ChatGPT-4o和o1才能完成训练。尽管模型蒸馏是一项常见的技术手段，能够将OpenAI中的大量数据迅速提炼重点并快速理解和应用，但这种模式只能让DeepSeek接近OpenAI，而难以真正超越OpenAI。

除了国外，国内关于DeepSeek V3曾在测试中出现过异常：该模型自称是OpenAI的ChatGPT，并能提供OpenAI的API使用说明。专家认为，这很可能是由于训练数据中混入了大量由ChatGPT生成的内容（即“蒸馏”数据），导致模型发生了“身份混淆”。

此外，**由中国科学院深圳先进技术研究院、北京大学、01.AI、南方科技大学、Leibowitz AI等多个知名机构的研究团队联合发表的《Distillation Quantification for Large Language Models（大语言模型的蒸馏量化）》论文则显示DeepSeek V3的蒸馏过程可能主要来自GPT4o，且蒸馏程度较高。**

该论文提出了一个系统化的框架，量化并评估大模型蒸馏的过程及其影响，采用了“响应相似性评估（RSE）”和“身份一致性评估（ICE）”两个量化指标。RSE实验结果显示，DeepSeek V3的蒸馏程度与GPT4o接近，评分为4.102，远高于其他模型（如Llama 3.1-70B和Doubao-Pro-32k）。在ICE实验中，DeepSeek V3也显示出较高的蒸馏程度，属于可疑响应数量最多的模型之一。

**对于蒸馏技术的争议，蔡恒进对TechWeb表示，DeepSeek完全蒸馏GPT的说法是错误的，ChatGPT的模型是闭源的，而DeepSeek展示了思考过程，这是抄不出来的。“DeepSeek技术上是有创新的，过于纠结蒸馏这方面完全没有意义。”蔡恒进说道。**

蔡恒进认为DeepSeek有着独特的技术路径优势，它可以从底层优化，可以绕开英伟达CUDA生态，可以大幅提升国产芯片做预训练的性能。

**打破算力魔咒  
未来算力还重要吗？**

基于我们前述成本的优势，有业内观点认为，DeepSeek的出现，打破了英伟达等科技巨头“堆积算力”的路径，也就是说，美国AI巨头们认定的那个靠钱、靠更高算力芯片才能堆出来的更好的模型，不需要那么高昂的门槛了。

**蔡恒进对TechWeb表示，原来我们一直认为不断“堆积算力”才能提高AI模型能力，但Deepseek的出现走出了另一条路，即不一定要提升很高的参数规模就能实现很高的性能，可能对算力需求至少降到10倍以上。“堆算力”本身没有错，但随着Deepseek的出现我们会发现这条路的性价比不高。**

DeepSeek-V3极低的训练成本预示着AI大模型对算力投入的需求将大幅下降，但也有观点认为，DeepSeek表现固然优秀，但其统计口径只计算了预训练，数据的配比需要做大量的预实验，合成数据的生成和清洗也需要消耗算力。

此外，在训练上做降本增效不代表算力需求会下降，只代表大厂可以用性价比更高的方式去做模型极限能力的探索。

业内在讨论算力时，常引用蒸汽时代的杰文斯悖论来类比。这一悖论由经济学家威廉·斯坦利·杰文斯提出，核心观点是：当某种资源的使用效率提高、获取变得更容易时，其总体使用量往往不减反增。以蒸汽机为例，燃油效率的提升降低了单位工作量所需的煤炭成本，反而刺激了更多的工业活动，导致煤炭的总体消耗量上升。蒸汽机效率的提升，不仅没有减少对蒸汽机的需求，反而因为技术的推广和应用场景的扩大，进一步增加了市场对蒸汽机的需求。DeepSeek 的发展也呈现出类似的趋势：算力效率的提升并未减少对算力的需求，反而推动了更多高算力应用的落地，使得行业对算力的需求持续增长。

对此，**中信证券研报也指出，近日，DeepSeek-V3的正式发版引起AI业内广泛高度关注，其在保证了模型能力的前提下，训练效率和推理速度大幅提升。DeepSeek新一代模型的发布意味着AI大模型的应用将逐步走向普惠，助力AI应用广泛落地；同时训练效率大幅提升，亦将助力推理算力需求高增。**

而Bloomberg Intelligence最近的一篇报告显示，企业客户可能会在2025年进行更大规模的AI投资，而AI支出增长将更侧重于推理侧，以实现投资变现或提升生产力。

那么上述存有争议的事实究竟如何？俗话说：让子弹再飞一会吧！

[qq](https://new.qq.com/rain/a/20250213A024LG00)
