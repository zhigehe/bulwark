---
title: "“算力崩塌”，是真是假"
date: "2025-02-08 09:05:42"
summary: "DeepSeek的开发成本极低，开源、服务完全免费，这让山姆·奥特曼和其他AI人工智能的从业者“印象..."
categories:
  - "iyiou"
lang:
  - "zh-CN"
translations:
  - "zh-CN"
tags:
  - "iyiou"
menu: ""
thumbnail: ""
lead: ""
comments: false
authorbox: false
pager: true
toc: false
mathjax: false
sidebar: "right"
widgets:
  - "search"
  - "recent"
  - "taglist"
---

DeepSeek的开发成本极低，开源、服务完全免费，这让山姆·奥特曼和其他AI人工智能的从业者“印象深刻”，让世界首富马斯克破防，让瑞·达利欧对美股深感担忧。

知名投资人“木头姐” 凯茜·伍德甚至直言：DeepSeek加剧了人工智能的成本崩溃。

“神秘的东方力量”让全世界为之侧目，也引发了中美AI领导地位更替的思考。不过，对于算力进行重新判断，可能仍然为时尚早。

当前，我国AI算力部署占全球算力基础设施的26%，名列世界第二。在“算力即国力”的思潮下，东数西算等数字基础设施工程正积极进行。

**DeepSeek颠覆了固有的“大力出奇迹”的大模型性能提升路径，短期内对算力需求预期会出现大幅下降。但长期来看，算力部署仍然有大规模需求扩张作的基础。**

正在进行的超大规模算力投资，对于中国科技产业来说，仍是一笔巨大的财富。Deepseek带来的对算力成本的大幅节省，与如今中国大规模投入的算力基础设施建设并不冲突。（本文是来自公众号《巨潮WAVE》内容团队的深度价值文章。）

### **颠覆**

DeepSeek的颠覆性创新，在于极致的效率革命。它仅用不到OpenAI十分之一的成本，就达到了后者最新大模型的性能。

1月20日，DeepSeek 正式发布DeepSeek-R1模型系列，大模型排行榜Chatbot Arena上，DeepSeek- R1的基准测试排名已升至全类别第三，与ChatGPT-4o最新版并驾齐驱，并在风格控制类模型分类中与OpenAI-o1并列头名。

![](https://diting-hetu.iyiou.com/async/weixin/6fhD7zbDZnQ4Gc5qajoz)

R1模型虽未公布训练成本，但据DeepSeekV3技术报告，V3模型的训练总计只需要278.8万GPU小时，相当于在2048块H800 (英伟达特供中国市场的低配版GPU)集群上训练约2个月，合计成本仅557.6万美金。

有传言，R1模型的训练成本基本相当，但参数规模达到惊人的6710亿，这些数据都足够令外界震惊。

作为对比，GPT-4o模型的训练成本约为1亿美元，需要英伟达GPU量级在万块以上，且是比H800性能更强的H100。

同样是开源模式的Meta Llama 3 系列模型，其训练需要多达3930万H100 GPU小时，DeepSeek的训练成本约相当于Llama 3的7%。也有AI大佬表示过，仅DeepSeek-V3级别的能力就需要接近16000颗GPU的集群。

也就是说，**这家中国初创AI公司仅用不到十分之一的成本，就达到了世界一流水平。**

这种颠覆性的成本优势，极有可能改变过往“高投入、高算力”的研发路径，市场对算力硬件需求持续高增长的预期或产生动摇。

“四两拨千斤”的能力源于其自研的MLA和MOE架构，为其自身的模型训练成本下降起到了关键作用。

此外，R1模型使用数据蒸馏技术（Distillation），通过一系列算法和策略，将原始复杂的数据进行去噪、降维、提炼，从而得到更为精炼、更为有用的数据，提升训练效率。

模型蒸馏可以通过训练小型模型模仿大型模型，广泛应用于提高AI效率与降低成本。比喻来说，DeepSeek相当于通过更高效的学习方法获得了优异分数，而OpenAI还在搞题海战术。

**OpenAI的训练非常依赖人工，其数据团队甚至分为不同水平的部门，大量数据标注还要转交给肯尼亚等廉价外包劳工，高维数据还需更高素质的专业人员进行标注，这些都是成本。**

近期OpenAI和一些舆论也在公开指责DeepSeek团队通过“模型蒸馏”技术“违规复制”其产品功能，但始终未提供具体证据。

**而且“数据越多性能越强”属于惯性固有思维，因为数据量越多，其中的干扰也将随之变大，在此之前，市场上已经有“人工智能变蠢了”的讨论出现。**

也就是说，如何对数据进行清洗和精炼，同样是提升模型能力的关键。通过创新训练方法，DeepSeek改变了堆砌算力的共识。

正如创始人梁文锋所说：“我们想去验证一些猜想。”

### **改写**

半导体领域普遍遵循摩尔定律，AGI行业则是沿着Scaling Law（模型规模定律）向前演进。

Scaling Law被业界认为是大模型训练的“第一性原理”，模型性能与规模（参数量、数据大小、算力资源）呈正相关——**参数越多、计算资源越大，模型的性能就越强。**

好比一个中学生，给他提供更丰富的学习资料、更长的学习时间和更好的学习环境，他的学习成绩普遍会更好。

此次AI浪潮正是以Scaling law为底层逻辑的技术革命，但DeepSeek的出现打破了这一定律，或者至少让Scaling law的边际效益出现放缓的迹象。

这带给业界的启示是，人工智能产业将不再一味追求大规模算力的投入，而是在模型架构和工程优化结合上进行突破。粗放式的疯狂投入发展阶段逐步退潮，AI创新进入追求效率，追求模型架构设计、工程优化全新阶段。

正如达摩院首席科学家赵德丽所认为的，**大模型可以看成是知识的压缩，怎么利用知识产生更好的智能，就是基于基础模型的思维推理必然发展的方向。**

如微软rStar-Math无需蒸馏，用60块A100训练的7B模型在数学推理上可媲美 OpenAI-o1；上海 AI 实验室的书生·浦语3.0，仅使用4T数据进行训练，综合性能超过了同量级的开源模型，而且训练成本降低了75%以上。‌

![](https://diting-hetu.iyiou.com/async/weixin/ehr7IByfd9ejyibsewXe)

DeepSeek带来的“范式转移”，不仅破除了科技大厂建立的技术领先壁垒，同时还打破了重资本比拼的游戏惯例。而且它不仅开源，而且还免费。

作为回应，OpenAI紧急上线新一代推理模型o3系列的mini版本，并且首次免费向用户开放其基础功能。奥特曼承认，**“我个人认为，在（开闭源）这个问题上我们站在了历史的错误一方，需要找出一个不同的开源策略。”**

效率优化策略，无疑给互联网大厂们的超级愿景泼了一盆冷水。

字节跳动2024年在AI赛道的资本开支就高达800亿元，接近BAT的总和；微软、谷歌、亚马逊、Meta、苹果五巨头合计资本开支2253亿美元，2025年有望继续增长19.6%。其中，单是微软就计划在2025财年砸下800亿美元，用于建设人工智能数据中心。

就在DeepSeek-R1模型发布两天后，美国政府就联合OpenAI、软银、甲骨文等启动“星际之门”项目，计划在未来四年内最高投资5000亿美元。孙正义还宣布要给OpenAI投资250亿美元，巨头们期望通过“军备竞赛”，维持自身在AI领域的全球领导地位。

此前，美国大厂为GPT-5、Llama4等下一代模型正使尽全力补充算力，奥特曼甚至一度去找到中东巨富，计划筹集7万亿美金建设一座超级数据中心和芯片制造厂。

DeepSeek的出现，改变了这种超大规模扩张算力的行业发展“固定路径”，但同时也出现了一种应该彻底放弃算力建设的声音。

### **过剩？**

国家算力战略部署的复杂性，显然会超过一般舆论讨论所得出的结论。尤其是，我国算力基础设施目前仍处于初步搭建阶段，还远未到过剩状态。

一方面，随着访问量急速飙升，用户蜂拥而至，DeepSeek深度思考和联网搜索功能也出现了宕机情况。DeepSeek移动应用上线仅18天就迎来了1600万次下载，几乎是ChatGPT同期的两倍，更是成为全球140个市场中下载量最高的APP。

另一方面，需要注意的是，**DeepSeek目前仅支持文字问答、读图、读文档等功能，还未涉及图片、音频和视频生成领域，未来要想突破文本范畴，其对算力和前期训练成本的投入都将呈几何级飙升。**

对于其他追随DeepSeek的大模型公司来说情况也是类似，随着用户的增长和产品线的不断丰富，最终都需要不断扩张算力。目前国内各大投资机构都在“疯狂对接”DeepSeek，显然是对此有非常清晰的判断。

百度CTO王海峰就认为，规模定律（Scaling Law）仍然有效，更高质量更大规模的数据、更大的模型将会带来更强的能力。

目前算力建设已成为国家级战略。2024年《政府工作报告》中就明确提出，适度超前建设数字基础设施，加快形成全国一体化算力体系。根据《中国综合算力指数（2024年）》报告，截至2023年末，我国算力基础设施规模占全球的26%，名列第二，仅次于美国。

工信部数据显示，截至2024年9月，我国算力总规模246EFLOPS（每秒进行百亿亿次浮点运算的能力），在用算力中心机架总规模超过830万标准机架。

![](https://diting-hetu.iyiou.com/async/weixin/Nn2dcNcjJ9eMAZPqk60x)

此前，六部门联合印发《算力基础设施高质量发展行动计划》，其中指出到2025年，我国算力规模将达到300EFLOPS，智能算力占比要达到35%。

“东数西算”工程早在2021年5月就已启动，京津冀、长三角、粤港澳大湾区、成渝、贵州、内蒙古、甘肃、宁夏八大枢纽和10大集群雏形已现。

![](https://diting-hetu.iyiou.com/async/weixin/Yd48r5rwYnb3MTg6w6wZ)

国内已建、正在建设的智算中心超过250个，规划具有超万张GPU集群的智算中心已有十多个，国产芯片厂商也因此受到极大关注。华为昇腾910B、寒武纪思元590、百度昆仓芯II-R200、海光信息深算二号，加上中芯国际，共同形成了对抗英伟达、AMD的中国军团。

这些大规模部署和研发投入，都具有历史性的战略意义，是人工智能时代的重要社会财富。

### **写在最后**

用低成本开发出优质产品，是中国在资源匮乏的历史条件下，实现工业现代化的独特文化，而美国则擅长所谓的“范佛里特弹药量”，倚仗超大规模投入获得超级领先地位。

在当前的世界经济格局中，中国家电、光伏等制造业基本都是以高性价比策略取胜，DeepSeek的成功，也是在另一个领域中体现出了中国智慧、巧思和韧性。

中美AI拉锯的混战中，原先还处在追赶阶段的中国企业，突然以一种领先或颠覆者的身份出现，让全球科技界大为吃惊。但我们仍需要保持冷静和谦逊的心态。

就像梁文锋所说的：

在美国每天发生的大量创新里，这是非常普通的一个。他们之所以惊讶，是因为这是一个中国公司，以创新贡献者的身份，加入到他们的游戏里去。

也正因如此，算力基础设施仍是一种人工智能时代不可或缺的创新土壤、社会资源。

[iyiou](https://www.iyiou.com/analysis/202502081089714)
