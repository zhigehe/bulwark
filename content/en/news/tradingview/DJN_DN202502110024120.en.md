---
title: "Why Do AI Chatbots Have Such a Hard Time Admitting 'I Don't Know'? — WSJ"
date: "2025-02-11 18:30:00"
summary: "By Ben Fritz\"Who is the journalist Ben Fritz married to?\"It's a simple question with a difficult-to-find answer, as there's virtually no information on the internet about my marriage. When I recently asked several of the world's most advanced artificial-intelligence chatbots, I got some bizarre responses: a writer I've never met;..."
categories:
  - "Dow Jones Newswires"
lang:
  - "en"
translations:
  - "en"
tags:
  - "Dow Jones Newswires"
menu: ""
thumbnail: ""
lead: ""
comments: false
authorbox: false
pager: true
toc: false
mathjax: false
sidebar: "right"
widgets:
  - "search"
  - "recent"
  - "taglist"
---

By Ben Fritz

"Who is the journalist Ben Fritz married to?"

It's a simple question with a difficult-to-find answer, as there's virtually no information on the internet about my marriage. When I recently asked several of the world's most advanced artificial-intelligence chatbots, I got some bizarre responses: a writer I've never met; a woman in Iowa I've never heard of; a tennis influencer.

Despite their ability to solve some of the world's most complex math problems and convincingly simulate human relationships, AI chatbots regularly get basic facts wrong. They invent legal cases, jumble the facts from famous movies and books, and, yes, make up spouses.

These wrong answers are known as hallucinations because AI apps like ChatGPT and Gemini express them with total confidence. As AI is integrated into our workplaces, schools and personal lives, they pose increasing risks for the people who use the technology. Researchers who once dismissed hallucinations as a relatively minor problem are now working on numerous potential fixes.

"It's one of the hottest fields for researchers," says Roi Cohen, an AI doctoral candidate at Germany's Hasso Plattner Institut who has interned at IBM and Microsoft.

AI models are designed to guess what word, or portion of a word, is most likely to come next in an answer. The whole process is, in a sense, highly educated guesswork. AIs are generally trained to give the best answer they can without betraying any doubt, much like students taking multiple-choice tests or guests blustering a response at a cocktail party.

"The original reason why they hallucinate is because if you don't guess anything, you don't have any chance of succeeding," says José Hernández-Orallo, a professor at Spain's Valencian Research Institute for Artificial Intelligence.

One solution researchers are trying, called retrieval augmented generation, searches the web or a library of documents to augment what an AI model already knows with freshly retrieved information. This ensures the AI has the best information on hand for the answer it generates. It's like looking through your library of photos from the past year before writing a holiday letter, rather than writing the whole thing from memory.

At NeurIPs, an annual conference in December for AI researchers in Vancouver, Cohen and Konstantin Dobler, a fellow doctoral student at the Hasso Plattner Institut, presented their own seemingly simple but novel idea: To root out the problem at a deeper level by teaching AI models to say three words they seem to hate: "I don't know."

AI models are created by having them ingest and analyze massive amounts of information, which these days typically means the entire public internet and whatever privately held materials a company can get its hands on. Very little of that information is about not knowing things, so the models don't inherently learn the value of politely throwing up their hands.

Cohen and Dobler designed a way to intervene during the earliest stages of developing an AI model, known as pretraining, to teach it about uncertainty. Their method increased the precision of answers an AI model gives by teaching it to say "I don't know" at least some of the time it would have confidently hallucinated.

Finding the perfect balance remains elusive, though. Some of the time when the AI said "I don't know," the correct answer actually was in the model's training data.

But for people who are looking to use AI in areas where accuracy matters, the trade-off would likely be worth it. "It's about having useful systems to deploy, even if they're not superintelligent," Dobler says.

Anthropic, the AI company behind the chatbot Claude, is already thinking along these lines. (Perhaps not coincidentally, Claude was also the only chatbot I tested that admitted it couldn't say who I'm married to.)

Rather than intervening when an AI is developed, as Cohen and Dobler proposed, Anthropic addresses the problem with its "system prompt," a set of behind-the-scenes instructions that shape the final steps of giving an answer. Claude's system prompt instructs the model that when people ask about niche information that would likely be difficult to find on the internet, it should warn them its answer might be a hallucination.

"Insofar as you can take that knowledge about its own limitations and try to get it to convey that, that to me looks like the best solution," says Amanda Askell, an Anthropic staffer who helps train Claude's personality.

Even as AI has gotten more powerful, Americans' faith in it has been decreasing. In 2023, 52% of people were more concerned than excited about AI, according to a Pew Research Center survey, compared with 37% in 2021.

Giving AI more modesty could be part of the solution.

"When you ask someone a difficult question and they say 'I cannot answer, ' I think that builds trust," the professor Hernández-Orallo observes. "We are not following that common-sense advice when we build AI."

Write to Ben Fritz at ben.fritz@wsj.com

[Dow Jones Newswires](https://www.tradingview.com/news/DJN_DN20250211002412:0/)
