---
title: "Open-source AI is definitely happening — the only question is how"
date: "2025-02-11 22:31:16"
summary: "If a powerful technology poses significant risks to business and society, should it ever be freely available? Many argue that AI falls into this category. Some even warn of existential threats. Since the advent of foundation models like ChatGPT, debates among AI experts, executives and regulators have centered around whether..."
categories:
  - "thehill"
lang:
  - "en"
translations:
  - "en"
tags:
  - "thehill"
menu: ""
thumbnail: ""
lead: ""
comments: false
authorbox: false
pager: true
toc: false
mathjax: false
sidebar: "right"
widgets:
  - "search"
  - "recent"
  - "taglist"
---

If a powerful technology poses significant risks to business and society, should it ever be freely available?

Many argue that AI falls into this category. Some even warn of existential threats. Since the advent of foundation models like ChatGPT, debates among AI experts, executives and regulators have centered around whether these models should be open-sourced.

But this has been the wrong focus all along. The emergence of [DeepSeek, and its creators’ decision to open-source an AI model almost on par with frontier models](https://www.livescience.com/technology/artificial-intelligence/why-is-deekspeek-such-a-game-changer-scientists-explain-how-the-ai-models-work-and-why-they-were-so-cheap-to-build) (for significantly cheaper), shifts the debate. The question is no longer “if” but “how” we can open-source AI — maximizing benefits while managing safety and misuse concerns.

Open-source AI takes the idea beyond just code to include data, algorithms and model weights — the learned parameters from training AI. A fully open-source AI system includes open datasets, open-source code and open model weights, but many organizations only release the model weights, which limits the ability to fully understand or rebuild the system. This becomes more complicated if the weights are trained on data that is not disclosed, potentially raising liability concerns. While openness can encourage innovation, it can also bring up questions about responsibility and security risks.

But the “unexpected” rise of DeepSeek could indicate that we may be on a one-way path for AI foundation models. The shift toward openness of these models, which can fuel applications very broadly and create financial value that can further support model improvements, may prove simply inevitable. Just like Linux became the foundation for much of the software we use today, open-source AI foundation models could soon become the standard for generative AI.

One of the strongest arguments for open-source AI is collective innovation. Publicly available models allow for global collaboration, accelerating breakthroughs beyond what any single team could achieve. Open models also can enhance transparency, allowing third parties to detect safety and security vulnerabilities — though the sheer scale of modern models makes this challenging.

Economic accessibility is another factor. Open AI lowers barriers to entry, enabling smaller firms, academia and governments — especially outside Silicon Valley — to compete. If managed correctly, this could help level the global economic playing field.

But openness can also amplify risks. Unrestricted access to AI models increases the potential for misuse, such as for new forms of cyberattacks. Unlike proprietary AI, which allows model providers to better manage usage, open-source models can be modified and repurposed freely, making misuse harder to track.

On the business side, open-source AI also threatens competitive advantages of model developers. Companies investing billions in training proprietary models may see their innovations commoditized, weakening funding and research incentives. Intellectual property concerns also arise: If an open model is later found to be trained on copyrighted material, who is responsible?

From a technical standpoint, quality control and accountability can become major concerns. Without centralized oversight, ensuring reliable AI is difficult, although risks can be mitigated at the application level.

AI relies on three key resources: data, computing power and mathematics. While debates focus on the first two, mathematical breakthroughs are the true driving force for much of the progress. Afterall, math has powered revolutions, from ancient engineering to modern AI. Progress in math, the algorithm, inevitably improves performance and efficiency. At the same time, in a competitive AI landscape, companies may strategically open-source models to disrupt rivals and gain an edge.

Therefore, restricting model access may ultimately be a futile endeavor.  The real surprise with DeepSeek then is that people were surprised by this.

If open AI is inevitable, governance must take priority. A responsible framework should focus on three key priorities: safety technologies, governance mechanisms and international policy alignment.

The rise of open AI necessitates better guardrail technologies — integrated safety features and real-time monitoring. Just as human-generated content is moderated online, AI-generated content must be managed responsibly. Investment in tools ensuring responsible AI use is essential.

AI governance should draw from best practices in internet security and software regulation. For example, model cards and data documentation can enhance transparency by detailing training data, intended use cases and limitations. Like SSL certificates for web security, a standardized AI safety certification could provide accountability without stifling innovation. Businesses adopting such standards may gain consumer trust while preempting regulatory scrutiny.

Finally, AI’s global implications demand some — even if minimal — global regulatory coordination. Europe’s AI Act offers one example for responsible AI development, but it must balance innovation with risks, as well as differences across regions. Countries outside the U.S. and China can now more easily embrace AI to bridge technological gaps, making international cooperation even more pressing.


[![](https://thehill.com/wp-content/uploads/sites/2/2023/11/op2.png?w=600)](https://thehill.com/submitting-opinion-content/)

Open-source AI, if governed effectively, could drive unprecedented value: accelerating innovation, enhancing global competition, and ensuring transparency and even safety. Interestingly, wider AI adoption may also provide the resources to continue the improvement of AI models outside closed walls, such as those of OpenAI. The challenge is to balance openness with risk management. By prioritizing governance, safety technologies and some international alignment, we can better ensure AI’s open future is as transformative as its advocates promise.

Meanwhile, the AI race is shifting from developing frontier models to applying them in the real world. DeepSeek may have marked a turning point, redirecting attention from who builds the most advanced models to how they are used. Whether businesses and regulators are ready for this shift remains to be seen, but one thing is certain: the AI landscape is changing, and the era of open AI is here to stay.

*Theodoros Evgeniou is professor of technology and business at INSEAD, where he also directs executive training programs on AI. Yann Lechelle is co-founder and CEO at Probabl.ai, a board member and VP Ecosystem at HUB France AI, and a member at One-o-One and JEDI. Fabio Valle is co-founder of the INSEAD AI community and VP of growth at Horsa.*

[thehill](https://thehill.com/opinion/5136602-open-source-ai-risks-benefits/)
